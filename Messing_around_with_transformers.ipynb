{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtHzytxobD2HEWcdNCouUG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/durml91/Personal/blob/main/Messing_around_with_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install equinox\n",
        "!pip install einops\n",
        "!pip install jaxtyping\n",
        "!pip install optax\n",
        "#!pip install jax==0.4.12\n",
        "!pip install transformers datasets[jax]"
      ],
      "metadata": {
        "id": "uw-lnHPiOktz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U jax jaxlib"
      ],
      "metadata": {
        "id": "WFrYg2RBY_EK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "p8hoGzcaOGJJ"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "from typing import Dict, List, Mapping, Optional\n",
        "\n",
        "import einops\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as jr\n",
        "import numpy as np\n",
        "import optax\n",
        "from datasets import load_dataset\n",
        "from jaxtyping import Array, Float, Int\n",
        "from tqdm import notebook as tqdm\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "import equinox as eqx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jax.devices(\"cpu\")[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPaYo44IYgGb",
        "outputId": "6a0bdb30-9761-424a-92d7-ba704712662c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CpuDevice(id=0)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "key = jr.PRNGKey(2023)"
      ],
      "metadata": {
        "id": "WztzSWkRWyvW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbedderBlock(eqx.Module):\n",
        "    token_embedder: eqx.nn.Embedding\n",
        "    segment_embedder: eqx.nn.Embedding\n",
        "    position_embedder: eqx.nn.Embedding\n",
        "    layernorm: eqx.nn.LayerNorm\n",
        "    dropout: eqx.nn.Dropout\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        max_length: int,\n",
        "        type_vocab_size: int,\n",
        "        embedding_size: int,\n",
        "        hidden_size: int,\n",
        "        dropout_rate: float,\n",
        "        key: jr.PRNGKey,\n",
        "    ):\n",
        "        token_key, segment_key, position_key = jr.split(key, 3)\n",
        "\n",
        "        self.token_embedder = eqx.nn.Embedding(\n",
        "            num_embeddings=vocab_size, embedding_size=embedding_size, key=token_key\n",
        "        )\n",
        "        self.segment_embedder = eqx.nn.Embedding(\n",
        "            num_embeddings=type_vocab_size,\n",
        "            embedding_size=embedding_size,\n",
        "            key=segment_key,\n",
        "        )\n",
        "        self.position_embedder = eqx.nn.Embedding(\n",
        "            num_embeddings=max_length, embedding_size=embedding_size, key=position_key\n",
        "        )\n",
        "        self.layernorm = eqx.nn.LayerNorm(shape=hidden_size)\n",
        "        self.dropout = eqx.nn.Dropout(dropout_rate)\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        token_ids: Int[Array , \" seq_len\"],\n",
        "        position_ids: Int[Array, \" seq_len\"],\n",
        "        segment_ids: Int[Array, \" seq_len\"],\n",
        "        enable_dropout: bool = False,\n",
        "        key: Optional[jr.PRNGKey] = None,\n",
        "    ) -> Float[Array, \"seq_len hidden_size\"]:\n",
        "        tokens = self.token_embedder(token_ids)\n",
        "        segments = self.segment_embedder(segment_ids)\n",
        "        positions = self.position_embedder(position_ids)\n",
        "        embedded_inputs = tokens + segments + positions\n",
        "        embedded_inputs = jax.vmap(self.layernorm)(embedded_inputs)\n",
        "        embedded_inputs = self.dropout(\n",
        "            embedded_inputs, inference=not enable_dropout, key=key\n",
        "        )\n",
        "        return embedded_inputs"
      ],
      "metadata": {
        "id": "UHvmVG73Ziye"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardBlock(eqx.Module):\n",
        "\n",
        "    mlp: eqx.nn.Linear\n",
        "    output: eqx.nn.Linear\n",
        "    layernorm: eqx.nn.LayerNorm\n",
        "    dropout: eqx.nn.Dropout\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size: int,\n",
        "        intermediate_size: int,\n",
        "        dropout_rate: float,\n",
        "        key: jr.PRNGKey,\n",
        "    ):\n",
        "        mlp_key, output_key = jr.split(key, 2)\n",
        "        self.mlp = eqx.nn.Linear(\n",
        "            in_features=hidden_size, out_features=intermediate_size, key=mlp_key\n",
        "        )\n",
        "        self.output = eqx.nn.Linear(\n",
        "            in_features=intermediate_size, out_features=hidden_size, key=output_key\n",
        "        )\n",
        "        self.layernorm = eqx.nn.LayerNorm(shape=hidden_size)\n",
        "        self.dropout = eqx.nn.Dropout(dropout_rate)\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        inputs: Float[Array, \"seq_len hidden_size\"],\n",
        "        enable_dropout: bool = True,\n",
        "        key: Optional[jr.PRNGKey] = None,\n",
        "    ) -> Float[Array, \"seq_len hidden_size\"]:\n",
        "\n",
        "        # Feed forward\n",
        "        hidden = self.mlp(inputs)\n",
        "        hidden = jax.nn.gelu(hidden)\n",
        "\n",
        "        # Project back to input size\n",
        "        output = self.output(hidden)\n",
        "        output = self.dropout(output, inference=not enable_dropout, key=key)\n",
        "\n",
        "        # Residual and layer norm\n",
        "        output += inputs\n",
        "        output = self.layernorm(output)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "d2ske7gAnnq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionBlock(eqx.Module):\n",
        "\n",
        "    attention: eqx.nn.MultiheadAttention\n",
        "    layernorm: eqx.nn.Embedding\n",
        "    dropout: eqx.nn.Dropout\n",
        "    num_heads: int = eqx.static_field()\n",
        "\n",
        "    def __init_(\n",
        "        self,\n",
        "        hidden_size: int,\n",
        "        num_heads: int,\n",
        "        dropout_rate: float,\n",
        "        attention_dropout_rate: float,\n",
        "        key: jr.PRNGKey,\n",
        "    ):\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = eqx.nn.MultiheadAttention(\n",
        "            num_heads=num_heads,\n",
        "            query_size=hidden_size,\n",
        "            use_query_bias=True,\n",
        "            use_key_bias=True,\n",
        "            use_value_bias=True,\n",
        "            use_output_bias=True,\n",
        "            dropout_p=attention_dropout_rate,\n",
        "            key=key,\n",
        "        )\n",
        "        self.layernorm = eqx.nn.LayerNorm(shape=hidden_size)\n",
        "        self.dropout = eqx.nn.Dropout(dropout_rate)\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        inputs: Float[Array, \"seq_len hidden_size\"],\n",
        "        mask: Optional[Int[Array, \" seq_len\"]],\n",
        "        enable_dropout: bool = False,\n",
        "        key: \"jr.PRNGKey\" = None,\n",
        "    ) -> Float[Array, \"seq_len hidden_size\"]:\n",
        "        if mask is not None:\n",
        "            mask = self.make_self_attention_mask(mask)\n",
        "        \n",
        "        attention_key, dropout_key = (\n",
        "            (None, None) if key is None else jr.split(key, 2)\n",
        "        )\n",
        "\n",
        "        attention_output = self.attention(\n",
        "            query=inputs,\n",
        "            key=inputs,\n",
        "            value=inputs,\n",
        "            mask=mask,\n",
        "            inference=not enable_dropout,\n",
        "            key=attention_key\n",
        "        )\n",
        "\n",
        "        result = attention_output\n",
        "        result = self.dropout(result, inference=not enable_dropout, key=dropout_key)\n",
        "        result = result + inputs\n",
        "        result = jax.vmap(self.layernorm)(result)\n",
        "        return result\n",
        "\n",
        "    def make_self_attention_mask(\n",
        "        self, mask: Int[Array, \" seq_len\"]\n",
        "    ) -> Float[Array, \"num_heads seq_len seq_len\"]:\n",
        "        \n",
        "        mask = jnp.multiply(\n",
        "            jnp.expand_dims(mask, axis=-1), jnp.expand_dims(mask, axis=-2)\n",
        "        )\n",
        "        mask = jnp.expand_dims(mask, axis=-3)\n",
        "        mask = jnp.repeat(mask, repeats=self.num_heads, axis=-3)\n",
        "        return mask.astype(jnp.float32)"
      ],
      "metadata": {
        "id": "A_ENK3Vo4U4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerLayer(eqx.Module):\n",
        "\n",
        "    attention_block: AttentionBlock\n",
        "    ff_block: FeedForwardBlock\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size: int,\n",
        "        intermidate_size: int,\n",
        "        num_heads: int,\n",
        "        dropout_rate: float,\n",
        "        attention_dropout_rate: float,\n",
        "        key: jr.PRNGKey,\n",
        "    ):\n",
        "\n",
        "        attention_key, ff_key = jr.split(key, 2)\n",
        "\n",
        "        self.attention_block = AttentionBlock(\n",
        "            hidden_size=hidden_size,\n",
        "            num_heads=num_heads,\n",
        "            dropout_rate=dropout_rate,\n",
        "            attention_dropout_rate=attention_dropout_rate,\n",
        "            key=attention_key,\n",
        "        )\n",
        "        self.ff_block = FeedForwardBlock(\n",
        "            hidden_size=hidden_size,\n",
        "            intermediate_size=intermediate_size,\n",
        "            dropout_rate=dropout_rate,\n",
        "            key=ff_key,\n",
        "        )\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        inputs: Float[Array, \"seq_len hidden_size\"],\n",
        "        mask: Optional[Int[Array, \" seq_len\"]] = None,\n",
        "        *,\n",
        "        enable_dropout: bool = False,\n",
        "        key: Optional[jr.PRNGKey] = None,\n",
        "    ) -> Float[Array, \"seq_len hidden_size\"]:\n",
        "        \n",
        "        attn_key, ff_key = (None, None) if key is None else jr.split(key)\n",
        "        attention_output = self.attention_block(\n",
        "            inputs, mask, enable_dropout=enable_dropout, key=attn_key\n",
        "        )\n",
        "\n",
        "        seq_len = inputs.shape[0]\n",
        "        ff_keys = None if ff_key is None else jr.split(ff_key, num=seq_len)\n",
        "        output = jax.vmap(self.ff_block, in_axes=(0, None, 0))(\n",
        "            attention_output, enable_dropout, ff_keys\n",
        "        )\n",
        "        return output"
      ],
      "metadata": {
        "id": "WyXTjhI_A4-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encode(eqx.Module):\n",
        "\n",
        "    embedder_block: EmbedderBlock\n",
        "    layers: List[TransformerLayer]\n",
        "    pooler: eqx.nn.linear\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        max_length: int,\n",
        "        type_vocab_size: int,\n",
        "        embedding_size: int,\n",
        "        hidden_size: int,\n",
        "        intermediate_size: int,\n",
        "        num_layers: int,\n",
        "        num_heads: int,\n",
        "        dropout_rate: float,\n",
        "        attention_dropout_rate: float,\n",
        "        key: jr.PRNGKey,\n",
        "    ):\n",
        "\n",
        "        embedder_key, layer_key, pooler_key = jr.split(key, 3)\n",
        "        self.embedder_block = EmbedderBlock(\n",
        "            vocab_size=vocab_size,\n",
        "            max_length=max_length,\n",
        "            type_vocab_size=type_vocab_size,\n",
        "            embedding_size=embedding_size,\n",
        "            hidden_size=hidden_size,\n",
        "            dropout_rate=dropout_rate,\n",
        "            key=embedder_key,\n",
        "        )\n",
        "\n",
        "        layer_keys = jr.split(layer_key, num=num_layers)\n",
        "        self.layers = []\n",
        "        for layer_key in layer_keys:\n",
        "            self.layers.append(\n",
        "                TransformerLayer(\n",
        "                    hidden_size=hidden_size,\n",
        "                    intermediate_size=intermediate_size,\n",
        "                    num_heads=num_heads,\n",
        "                    dropout_rate=dropout_rate,\n",
        "                    attention_dropout_rate=attention_dropout_rate,\n",
        "                    key=layer_key,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.pooler = eqx.nn.Linear(\n",
        "            in_features=hidden_size, out_features=hidden_size, key=pooler_key\n",
        "        )\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        token_ids: Int[Array, \" seq_len\"],\n",
        "        position_ids: Int[Array, \" seq_len\"],\n",
        "        segment_ids: Int[Array, \" seq_len\"],\n",
        "        *,\n",
        "        enable_dropout: bool = False,\n",
        "        key: Optional[jr.PRNGKey] = None,        \n",
        "    ) -> Dict[str, Array]:\n",
        "\n",
        "        emb_key, l_key = (None, None) if key is None else jr.split(key)\n",
        "\n",
        "        embeddings = self.embedder_block(\n",
        "            token_ids=token_ids,\n",
        "            position_ids=position_ids,\n",
        "            segment_ids=segment_ids,\n",
        "            enable_dropout=enable_dropout,\n",
        "            key=emb_key,\n",
        "        )\n",
        "\n",
        "        # take away all zero values\n",
        "        mask = jnp.asarray(token_ids != 0, dtype=jnp.int32)\n",
        "\n",
        "        x = embeddings\n",
        "        layer_outputs = []\n",
        "        for layer in self.layers:\n",
        "            cl_key, l_key = (None, None) if l_key is None else jr.split(l_key)\n",
        "            x = layer(x, mask, enable_dropout=enable_dropout, key=cl_key)\n",
        "            layer_outputs.append(x)\n",
        "\n",
        "        # BERT pooling\n",
        "        first_token_last_layer = x[..., 0, :]\n",
        "        pooled = self.pooler(first_token_last_layer)\n",
        "        pooled=jnp.tanh(pooled)\n",
        "\n",
        "        return(\"embeddings\": embeddings, \"layers\": layer_outputs, \"pooled\": pooled)\n"
      ],
      "metadata": {
        "id": "ekHyJG2OHrpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BertClassifier(eqx.Module):\n",
        "\n",
        "    encoder: Encoder\n",
        "    classifier_head: eqx.nn.Linear\n",
        "    dropout: eqx.nn.Dropout\n",
        "\n",
        "    def __init__(self, config: Mapping, num_classes: int, key: jr.PRNGKey):\n",
        "        \n",
        "        encoder_key, head_key = jr.split(key, 2)\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            vocab_size=config[\"vocab_size\"],\n",
        "            max_length=config[\"max_position_embeddings\"],\n",
        "            type_vocab_size=config[\"type_vocab_size\"],\n",
        "            embedding_size=config[\"hidden_size\"],\n",
        "            hidden_size=config[\"hidden_size\"],\n",
        "            intermediate_size=config[\"hidden_size\"],\n",
        "            num_layers=config[\"num_hidden_layers\"],\n",
        "            num_heads=config[\"num_attention_heads\"],\n",
        "            dropout_rate=config[\"hidden_dropout_prob\"],\n",
        "            attention_dropout_rate=config[\"attention_probs_dropout_prob\"],\n",
        "            key=encoder_key,\n",
        "        )\n",
        "        self.classifier_head = eqx.nn.Linear(\n",
        "            in_features=config[\"hidden_size\"], out_features=num_classes, key=head_key\n",
        "        )\n",
        "        self.dropout = eqx.nn.Dropout(config[\"hidden_dropout_prob\"])\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        inputs: Dict[str, Int[Array, \" seq_len\"]],\n",
        "        emable_dropout: bool = True,\n",
        "        key: jr.PRNGKey = None,\n",
        "    ) -> Float[Array, \" num_classes\"]:\n",
        "        \n",
        "        seq_len = inputs[\"token_ids\"].shape[-1]\n",
        "        position_ids = jnp.arange(seq_len)\n",
        "\n",
        "        e_key, d_key = (None, None) if key is None else jr.split(key)\n",
        "\n",
        "        pooled_output = self.encoder(\n",
        "            token_ids=inputs[\"token_ids\"],\n",
        "            segment_ids=inputs[\"segment_ids\"],\n",
        "            position_ids=position_ids,\n",
        "            enable_dropout=enable_dropout,\n",
        "            key=e_key,\n",
        "        )[\"pooled\"]\n",
        "        pooled_output = self.dropout(\n",
        "            pooled_output, inference=not enable_dropout, key=d_key\n",
        "        )\n",
        "\n",
        "        return self.classifier_head(pooled_output)"
      ],
      "metadata": {
        "id": "0LdjTX9MZBpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_config = {\n",
        "    \"vocab_size\": 30522,\n",
        "    \"hidden_size\": 128,\n",
        "    \"num_hidden_layers\": 2,\n",
        "    \"num_attention_heads\": 2,\n",
        "    \"hidden_act\": \"gelu\",\n",
        "    \"intermediate_size\": 512,\n",
        "    \"hidden_dropout_prob\": 0.1,\n",
        "    \"attention_probs_dropout_prob\": 0.1,\n",
        "    \"max_position_embeddings\": 512,\n",
        "    \"type_vocab_size\": 2,\n",
        "    \"initializer_range\": 0.02,\n",
        "}\n",
        "\n",
        "key = jax.random.PRNGKey(5678)\n",
        "model_key, train_key = jax.random.split(key)\n",
        "classifier = BertClassifier(config=bert_config, num_classes=2, key=model_key)"
      ],
      "metadata": {
        "id": "1wetnjoUrcUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "https://github.com/patrick-kidger/equinox/blob/main/examples/bert_checkpoint.eqx\n",
        "classifier_chkpt = eqx.tree_deserialise_leaves(\"bert_checkpoint.eqx\", classifier)"
      ],
      "metadata": {
        "id": "WwEI3KXMrslj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"google/bert_uncased_L-2_H-128_A-2\", model_max_length=128\n",
        ")\n",
        "\n",
        "def tokenize(example):\n",
        "    return tokenizer(example[\"sentence\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "ds = load_dataset(\"sst2\")\n",
        "ds = ds.map(tokenize, batched=True)\n",
        "ds.set_format(type=\"jax\", columns=[\"input_ids\", \"token_type_ids\", \"label\"])"
      ],
      "metadata": {
        "id": "336QOOCvr1pM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@eqx.filter_value_and_grad\n",
        "def compute_loss(classifier, inputs, key):\n",
        "    batch_size = inputs[\"token_ids\"].shape[0]\n",
        "    batched_keys = jr.split(key, num=batch_size)\n",
        "    logits = jax.vmap(classifier, in_axes=(0, None, 0))(inputs, True, batched_keys)\n",
        "    return jnp.mean(\n",
        "        optax.softmax_cross_entropy_with_integer_labels(\n",
        "            logits=logits, labels=inputs[\"label\"]\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "def make_step(model, inputs, opt_state, key, tx):\n",
        "    \n",
        "    key, new_key = jr.split(key)\n",
        "    loss, grads = compute_loss(model, inputs, key)\n",
        "    grads = jax.lax.pmean(grads, axis_name=\"devices\")\n",
        "\n",
        "    updates, opt_state = tx.update(grads, opt_state, model)\n",
        "    model = eqx.apply_updates(model, updates)\n",
        "\n",
        "    return loss, model, opt_state, new_key\n",
        "\n",
        "\n",
        "def make_eval_step(model, inputs):\n",
        "    return jax.vmap(functools.partial(model, enable_dropout=False))(inputs)\n",
        "  \n",
        "p_make_eval_step = eqx.filter.pmap(make_eval_step)"
      ],
      "metadata": {
        "id": "EN8xKA-gsp0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 3\n",
        "batch_size = 32\n",
        "learning_rate = 1e-5\n",
        "\n",
        "num_devices = jax.device_count()\n",
        "assert (\n",
        "    batch_size % num_devices == 0\n",
        ")\n",
        "\n",
        "tx = optax.adam(learning_rate=learning_rate)\n",
        "tx = optax.chain(optax.clip_by_global_norm(1.0), tx)\n",
        "opt_state = tx.init(classifier_chkpt)\n",
        "\n",
        "p_make_step = eqx.filter_pmap(functools.partial(make_step, tx=tx), axis_name='devices')\n"
      ],
      "metadata": {
        "id": "_n_ZkX0LueKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt_state = jax.device_put_replicated(opt_state, jax.local_devices())\n",
        "model = jax.device_put_replicated(classifier_chkpt, jax.local_devices())\n",
        "train_key = jax.device_put_replicated(train_key, jax.local_devices())"
      ],
      "metadata": {
        "id": "rhqdnx7A4xxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    with tqdm.tqdm(\n",
        "        ds[\"train\"].iter(batch_size=batch_size, drop_last_batch=True),\n",
        "        total=ds[\"train\"].num_rows // batch_size,\n",
        "        unit=\"steps\",\n",
        "        desc=f\"Epoch {epoch+1}/{epochs}\",        \n",
        "    ) as tqdm_epoch:\n",
        "      for batch in tqdm_epoch:\n",
        "          token_ids, token_type_ids = batch[\"input_ids\"], batch[\"token_type_ids\"]\n",
        "          label = batch[\"label\"]\n",
        "\n",
        "          token_ids = einops.rearrange(\n",
        "              token_ids, \"(b1 b2) s -> b1 b2 s\", b1=num_devices\n",
        "          )\n",
        "\n",
        "          token_type_ids = einops.rearrange(\n",
        "              token_type_ids, \"(b1 b2) s -> b1 b2 s\", b1=num_devices\n",
        "          )\n",
        "\n",
        "          label = einops.rearrange(label, \"(b1 b2) -> b1 b2\", b1=num_devices)\n",
        "\n",
        "          inputs = {\n",
        "              \"token_ids\" : token_ids,\n",
        "              \"segment_ids\" : token_type_ids,\n",
        "              \"label\" : label,\n",
        "          }\n",
        "          loss, model, opt_state, train_key = p_make_step(\n",
        "              model, inputs, opt_state_train_key\n",
        "          )\n",
        "\n",
        "          tqdm_eopc.set_postfix(loss=np.sum(loss).item())"
      ],
      "metadata": {
        "id": "vgG27i2X5SN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = []\n",
        "\n",
        "for batch in tqdm.tqdm(\n",
        "    ds[\"validation\"].iter(batch_size=batch_size),\n",
        "    units=\"steps\",\n",
        "    total=np.ceil(ds[\"validation\"].num_rows / batch_size)   #ceiling function - round up to nearest integer\n",
        "    desc='Validation',\n",
        "):\n",
        "\n",
        "    token_ids, token_type_ids = batch[\"input_ids\"], batch[\"token_type_ids\"]\n",
        "    label = batch[\"label\"]\n",
        "\n",
        "    #unnecessary device split\n",
        "    token_ids = einops.rearrange(token_ids, \"(b1 b2) s -> b1 b2 s\", b1=num_devices)\n",
        "    token_type_ids = einops.rearrange(\n",
        "        token_type_ids, \"(b1 b2) s -> b1 b2 s\", b1=num_devices\n",
        "    )\n",
        "\n",
        "    inputs = {\"token_ids\": token_ids, \"segment_ids\": token_type_ids}\n",
        "\n",
        "    \n",
        "    output = p_make_eval_step(model, inputs)\n",
        "    output = map(float, np.argmax(output.reshape(-1, 2), axis=-1) == label)\n",
        "    outputs.extend(output)\n",
        "\n",
        "print(f\"Accuracy: {1== * np.sum(outputs) / len(outputs):/2f}%\")"
      ],
      "metadata": {
        "id": "XK1dPBXf_f2X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}