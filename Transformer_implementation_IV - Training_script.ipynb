{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPcneUXSb4cn5A+v25bOARr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/durml91/Personal/blob/main/Transformer_implementation_IV%20-%20Training_script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Basic Transformer (Encoder-Decoder) Implementation**"
      ],
      "metadata": {
        "id": "E0Fb9_WFlx9S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installs and Imports"
      ],
      "metadata": {
        "id": "DTp1uP3Bl8mk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Nvidia CUDA version check*"
      ],
      "metadata": {
        "id": "Nnep2ZGcgnGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !nvcc --version\n",
        "# !cat /usr/local/cuda/version.txt\n",
        "# !cat /usr/include/x86_64-linux-gnu/cudnn_v*.h | grep CUDNN_MAJOR -A 2"
      ],
      "metadata": {
        "id": "sPV3QWxdc3kH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install datasets before updating JAX**"
      ],
      "metadata": {
        "id": "1SmdbV4FmBYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets[jax]\n",
        "!pip install tokenizers\n",
        "!pip install transformers\n",
        "!pip install einops\n",
        "!pip install equinox\n",
        "!pip install torchmetrics"
      ],
      "metadata": {
        "id": "AtfVbSgemT53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jaxlib==0.4.13+cuda11.cudnn86 -f  https://storage.googleapis.com/jax-releases/jax_cuda_releases.html # [cuda]\n",
        "\n",
        "#!pip install -U jax jaxlib # [cpu]"
      ],
      "metadata": {
        "id": "b-qT1_q-fG9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Restart runtime here!*"
      ],
      "metadata": {
        "id": "0vDHzk5ymU-V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgYFrPvSpbjk"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.random as jr\n",
        "import jax.numpy as jnp\n",
        "\n",
        "import einops\n",
        "\n",
        "import equinox as eqx\n",
        "\n",
        "import optax\n",
        "\n",
        "import tqdm\n",
        "from tqdm import notebook as tqdm\n",
        "\n",
        "import functools\n",
        "\n",
        "from typing import Dict, List, Mapping, Optional, Callable\n",
        "from jaxtyping import Array, Float, Int\n",
        "\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "from matplotlib.pylab import plt\n",
        "\n",
        "from torchmetrics.text import WordErrorRate, CharErrorRate, BLEUScore\n",
        "from torch.utils import data\n",
        "\n",
        "from pickle import dump, load\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Check VM is using GPU accelerator*"
      ],
      "metadata": {
        "id": "svWX2qPyhPsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jax.default_backend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "kG9vgAmUbj-l",
        "outputId": "b39e862b-aabd-4c38-d12f-bb6d0c9203b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'gpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random key generation**"
      ],
      "metadata": {
        "id": "o-t84lDvmrRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rand_key = jr.PRNGKey(2023)"
      ],
      "metadata": {
        "id": "52URpWXEprjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer"
      ],
      "metadata": {
        "id": "P6oW6sooqpX1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generic equinox module for GELU**"
      ],
      "metadata": {
        "id": "zokOJ5im3RfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Lambda(eqx.Module):\n",
        "\n",
        "    fn: Callable\n",
        "\n",
        "    def __call__(self, x, *, key=None):\n",
        "        return self.fn(x)"
      ],
      "metadata": {
        "id": "kKw12VA93Q1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Attention**"
      ],
      "metadata": {
        "id": "ZX585wtq5F0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionBlock(eqx.Module):\n",
        "\n",
        "    attention: eqx.nn.MultiheadAttention\n",
        "    dropout: eqx.nn.Dropout\n",
        "    num_heads: int = eqx.field(static=True)\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        num_heads: int,\n",
        "        dropout_rate: float,\n",
        "        attention_dropout_rate: float,\n",
        "        key: jr.PRNGKey\n",
        "    ):\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = eqx.nn.MultiheadAttention(\n",
        "            num_heads=num_heads,\n",
        "            query_size=d_model,\n",
        "            use_query_bias=True,\n",
        "            use_key_bias=True,\n",
        "            use_value_bias=True,\n",
        "            use_output_bias=True,\n",
        "            dropout_p=attention_dropout_rate,\n",
        "            key=key,\n",
        "        )\n",
        "\n",
        "        self.dropout = eqx.nn.Dropout(dropout_rate)\n",
        "\n",
        "    @eqx.filter_jit\n",
        "    def causal_attention_mask(self, q_shape, kv_shape, pad: bool):\n",
        "        if pad == True:\n",
        "            #print(\"causal - padding\")\n",
        "            mask = jnp.ones((q_shape, kv_shape), dtype=float)\n",
        "            mask = jnp.tril(mask, k=0)\n",
        "            return mask == 1\n",
        "        else:\n",
        "            #print(\"causal - no padding\")\n",
        "            mask = jnp.ones((q_shape, kv_shape), dtype=float)\n",
        "            mask = jnp.tril(mask, k=0) == 1\n",
        "            mask = jnp.expand_dims(mask, axis=-3)\n",
        "            mask = jnp.where(mask, 1.0, 0.0)\n",
        "            return jnp.repeat(mask, repeats=self.num_heads, axis=-3)\n",
        "\n",
        "\n",
        "    @eqx.filter_jit\n",
        "    def make_attention_mask(\n",
        "        self, q_pad_mask: Array = jnp.array(()), kv_pad_mask: Array = jnp.array(()), look_ahead_mask: bool = False, inf_shape: int = None\n",
        "    ) -> Float[Array, \"num_heads query_seq_len kv_seq_len\"]:\n",
        "\n",
        "        if q_pad_mask is None:\n",
        "            #print(\"cross attn for inf\")\n",
        "            mask = einops.repeat(kv_pad_mask, \" a b -> (a q) b\", q = inf_shape)\n",
        "            mask = jnp.where(mask, 1.0, 0.0)\n",
        "            mask = jnp.expand_dims(mask, axis=-3)\n",
        "            mask = jnp.repeat(mask, repeats=self.num_heads, axis=-3)\n",
        "            return mask\n",
        "\n",
        "        # creates rectangular padding with q_pad_mask on the vertical and kv_pad_mask on the horizontal!\n",
        "        padding_mask = jnp.multiply(jnp.expand_dims(q_pad_mask, axis=-1), jnp.expand_dims(kv_pad_mask, axis=-2))\n",
        "        p_mask = jnp.where(padding_mask, 1.0, 0.0)\n",
        "        p_mask = jnp.repeat(p_mask, repeats=self.num_heads, axis=-3)\n",
        "        if look_ahead_mask is True:\n",
        "            q_shape = q_pad_mask.shape[-1]\n",
        "            kv_shape = kv_pad_mask.shape[-1]\n",
        "            cross_attn_mask = padding_mask & jnp.expand_dims(self.causal_attention_mask(q_shape, kv_shape, pad = True), axis=-3)\n",
        "            cross_attn_mask = jnp.where(cross_attn_mask, 1.0, 0.0)\n",
        "            cross_attn_mask = jnp.repeat(cross_attn_mask, repeats=self.num_heads, axis=-3)\n",
        "            return cross_attn_mask   # .astype(jnp.float32)\n",
        "        else:\n",
        "            return p_mask  # .astype(jnp.float32)\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        query: Float[Array, \"... d_model\"],\n",
        "        key_: Float[Array, \"... d_model\"],\n",
        "        value: Float[Array, \"... d_model\"],\n",
        "        q_pad_mask: Array = None,\n",
        "        kv_pad_mask: Array = None,\n",
        "        look_ahead_mask: bool = False,\n",
        "        enable_dropout: bool = False,\n",
        "        key: \"jr.PRNGKey\" = None,\n",
        "    ) -> Float[Array, \"... d_model\"]:\n",
        "\n",
        "\n",
        "        if q_pad_mask is not None and kv_pad_mask is not None:\n",
        "            #print(\"train\")\n",
        "            mask = self.make_attention_mask(q_pad_mask, kv_pad_mask, look_ahead_mask)\n",
        "        elif q_pad_mask is None and look_ahead_mask == True:\n",
        "            #print(\"self_attn - inference\")\n",
        "            mask = self.causal_attention_mask(query.shape[-2], key_.shape[-2], pad = False)\n",
        "        elif q_pad_mask is None and kv_pad_mask is not None:\n",
        "            #print(\"cross_attn - inference\")\n",
        "            mask = self.make_attention_mask(q_pad_mask, kv_pad_mask, inf_shape = query.shape[-2])\n",
        "        else:\n",
        "            #print(\"none\")\n",
        "            mask = None\n",
        "\n",
        "        #print(mask)\n",
        "        #print(jnp.sum(mask))\n",
        "\n",
        "\n",
        "        attention_key, dropout_key = (\n",
        "            (None, None) if key is None else jr.split(key)\n",
        "        )\n",
        "\n",
        "        attention_output = self.attention(\n",
        "              query=query,\n",
        "              key_=key_,\n",
        "              value=value,\n",
        "              mask=mask,\n",
        "              inference=not enable_dropout,\n",
        "              key=attention_key\n",
        "        )\n",
        "\n",
        "        att_drop = self.dropout(attention_output, inference=not enable_dropout, key=dropout_key)\n",
        "\n",
        "        return att_drop"
      ],
      "metadata": {
        "id": "0PShmTm65HGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MLP Block**"
      ],
      "metadata": {
        "id": "BsMLjw_gEfsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardBlock(eqx.Module):\n",
        "\n",
        "    mlp: eqx.nn.Sequential    #could also use MLP if this way is more fiddly\n",
        "    dropout: eqx.nn.Dropout\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        intermediate_size: int,\n",
        "        dropout_rate: float,\n",
        "        key: jr.PRNGKey,\n",
        "    ):\n",
        "        linear1, linear2 = jr.split(key)\n",
        "\n",
        "        self.mlp = eqx.nn.Sequential([\n",
        "            eqx.nn.Linear(in_features=d_model, out_features=intermediate_size, key=linear1),\n",
        "            Lambda(jax.nn.gelu),\n",
        "            eqx.nn.Linear(in_features=intermediate_size, out_features=d_model, key=linear2)\n",
        "        ])\n",
        "        self.dropout = eqx.nn.Dropout(dropout_rate)\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        inputs: Float[Array, \"... d_model\"],\n",
        "        enable_dropout: bool = True,\n",
        "        key: Optional[jr.PRNGKey] = None,\n",
        "    ) -> Float[Array, \"seq_len d_model\"]:\n",
        "\n",
        "        feed_out = jax.vmap(self.mlp)(inputs)\n",
        "        out_d = self.dropout(feed_out, inference=not enable_dropout, key=key)\n",
        "\n",
        "        return out_d\n"
      ],
      "metadata": {
        "id": "Npn_D1FBEOTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Embedding table**"
      ],
      "metadata": {
        "id": "zvVB7q4OSMvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InputEmbeddings(eqx.Module):\n",
        "\n",
        "  embedding: eqx.nn.Embedding\n",
        "  d_model: int\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      d_model: int,\n",
        "      vocab_size: int,\n",
        "      key: jr.PRNGKey,\n",
        "  ):\n",
        "      self.d_model = d_model\n",
        "      self.embedding = eqx.nn.Embedding(vocab_size, d_model, key=key)\n",
        "\n",
        "  def __call__(self, x) -> Float[Array, \"seq_len d_model\"]:\n",
        "\n",
        "    return self.embedding(x) * math.sqrt(self.d_model)"
      ],
      "metadata": {
        "id": "gW98v6F7qjxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positional encoding**"
      ],
      "metadata": {
        "id": "eGzfSNHsSOb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SinusoidalPosEmb(eqx.Module):\n",
        "    pos_emb: jax.Array\n",
        "    dropout: eqx.nn.Dropout\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        seq_len: int,\n",
        "        dropout_rate: float\n",
        "    ):\n",
        "        self.dropout = eqx.nn.Dropout(dropout_rate)\n",
        "        half_dim = d_model//2\n",
        "        pe = jnp.zeros((seq_len, d_model))\n",
        "        position = einops.repeat(jnp.arange(0 , seq_len), \" s -> s r\", r=half_dim)  #shape [seq_len, d_model/2]\n",
        "        div_term = jnp.exp(jnp.arange(0, d_model, 2) * -(math.log(10_000) / d_model))  #shape [d_model/2]\n",
        "        ins = jax.vmap(jnp.multiply, in_axes=(1, 0), out_axes=1)(position, div_term)  #shape [seq_len, d_model/2]\n",
        "\n",
        "        pe = pe.at[:, 0::2].set(jnp.sin(ins))\n",
        "        pe = pe.at[:, 1::2].set(jnp.cos(ins))\n",
        "\n",
        "        self.pos_emb = pe\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        x,\n",
        "        enable_dropout: bool = False,\n",
        "        key: \"jr.PRNGKey\" = None,\n",
        "    ) -> Float[Array, \"seq_len d_model\"]:\n",
        "\n",
        "        x = x + jax.lax.stop_gradient(self.pos_emb[:x.shape[0], :])\n",
        "\n",
        "        return self.dropout(x, inference=not enable_dropout, key=key)"
      ],
      "metadata": {
        "id": "Ua8tZGvG7pe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encoder Block**"
      ],
      "metadata": {
        "id": "kPL-N55ntQmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(eqx.Module):\n",
        "\n",
        "    layer_norm_1: eqx.nn.LayerNorm\n",
        "    layer_norm_2: eqx.nn.LayerNorm\n",
        "    attention_block: AttentionBlock\n",
        "    ff_block: FeedForwardBlock\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        intermediate_size: int,\n",
        "        num_heads: int,\n",
        "        dropout_rate: float,\n",
        "        attention_dropout_rate: float,\n",
        "        key: jr.PRNGKey,\n",
        "    ):\n",
        "\n",
        "        attention_key, ff_key = jr.split(key)\n",
        "\n",
        "        self.attention_block = AttentionBlock(\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            dropout_rate=dropout_rate,\n",
        "            attention_dropout_rate=attention_dropout_rate,\n",
        "            key=attention_key,\n",
        "        )\n",
        "        self.ff_block = FeedForwardBlock(\n",
        "            d_model=d_model,\n",
        "            intermediate_size=intermediate_size,\n",
        "            dropout_rate=dropout_rate,\n",
        "            key=ff_key,\n",
        "        )\n",
        "        self.layer_norm_1 = eqx.nn.LayerNorm(shape=d_model)\n",
        "        self.layer_norm_2 = eqx.nn.LayerNorm(shape=d_model)\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        inputs: Float[Array, \"src_seq_len d_model\"],\n",
        "        pad_mask: Array = None,\n",
        "        *,\n",
        "        enable_dropout: bool = False,\n",
        "        key: Optional[jr.PRNGKey] = None,\n",
        "    ) -> Float[Array, \"seq_len d_model\"]:\n",
        "\n",
        "        attn_key, ff_key = (None, None) if key is None else jr.split(key)\n",
        "\n",
        "        inputs = inputs + self.attention_block(\n",
        "            inputs, inputs, inputs, q_pad_mask=pad_mask, kv_pad_mask=pad_mask, enable_dropout=enable_dropout, key=attn_key\n",
        "        )\n",
        "        inputs = jax.vmap(self.layer_norm_1)(inputs)\n",
        "\n",
        "        inputs = inputs + self.ff_block(\n",
        "            inputs, enable_dropout=enable_dropout, key=ff_key\n",
        "        )\n",
        "        inputs = jax.vmap(self.layer_norm_2)(inputs)\n",
        "\n",
        "        return inputs"
      ],
      "metadata": {
        "id": "s_zpRdXMtP9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encoder**"
      ],
      "metadata": {
        "id": "7ZDEK-86ym-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(eqx.Module):\n",
        "\n",
        "    embedder_block: InputEmbeddings\n",
        "    pos_embed: SinusoidalPosEmb\n",
        "    layers: List[EncoderLayer]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        d_model: int,\n",
        "        seq_len: int,\n",
        "        intermediate_size: int,\n",
        "        num_layers: int,\n",
        "        num_heads: int,\n",
        "        dropout_rate: float,\n",
        "        attention_dropout_rate: float,\n",
        "        key: jr.PRNGKey,\n",
        "    ):\n",
        "\n",
        "      embedder_key, layer_key = jr.split(key, num=2)\n",
        "\n",
        "      self.embedder_block = InputEmbeddings(\n",
        "          d_model, vocab_size, key=embedder_key\n",
        "      )\n",
        "      self.pos_embed = SinusoidalPosEmb(d_model, seq_len, dropout_rate)\n",
        "\n",
        "      layer_keys = jr.split(layer_key, num=num_layers)\n",
        "\n",
        "      self.layers = [\n",
        "          EncoderLayer(\n",
        "              d_model=d_model, intermediate_size=intermediate_size, num_heads=num_heads, dropout_rate=dropout_rate, attention_dropout_rate=attention_dropout_rate, key=layer_key,\n",
        "          )\n",
        "          for layer_key in layer_keys]\n",
        "\n",
        "    def __call__(\n",
        "          self,\n",
        "          tokens: Int[Array, \"src_seq_len\"],\n",
        "          pad_mask: Array = None,\n",
        "          *,\n",
        "          enable_dropout: bool = False,\n",
        "          key: Optional[jr.PRNGKey] = None,\n",
        "    ):\n",
        "\n",
        "          ps_emb_key, l_key = (None, None) if key is None else jr.split(key, 2)\n",
        "\n",
        "          embed_inputs = self.embedder_block(tokens)\n",
        "          x = self.pos_embed(embed_inputs, enable_dropout=enable_dropout, key=ps_emb_key)\n",
        "          for layer in self.layers:\n",
        "              cl_key, l_key = (None, None) if l_key is None else jr.split(l_key)\n",
        "\n",
        "              x = layer(x, pad_mask=pad_mask, enable_dropout=enable_dropout, key=cl_key)\n",
        "\n",
        "          return x"
      ],
      "metadata": {
        "id": "3Szh7x1sylML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decoder Block**"
      ],
      "metadata": {
        "id": "7Ayup6V6FxC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(eqx.Module):\n",
        "\n",
        "    self_att_block: AttentionBlock\n",
        "    cross_att_block: AttentionBlock\n",
        "    ff_block: FeedForwardBlock\n",
        "    layer_norm_1: eqx.nn.LayerNorm\n",
        "    layer_norm_2: eqx.nn.LayerNorm\n",
        "    layer_norm_3: eqx.nn.LayerNorm\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        intermediate_size: int,\n",
        "        num_heads: int,\n",
        "        dropout_rate: float,\n",
        "        attention_dropout_rate: float,\n",
        "        key: jr.PRNGKey,\n",
        "    ):\n",
        "\n",
        "        self_att_key, cross_att_key, ff_key = jr.split(key, num=3)\n",
        "\n",
        "        self.self_att_block = AttentionBlock(\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            dropout_rate=dropout_rate,\n",
        "            attention_dropout_rate=attention_dropout_rate,\n",
        "            key=self_att_key,\n",
        "        )\n",
        "        self.cross_att_block = AttentionBlock(\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            dropout_rate=dropout_rate,\n",
        "            attention_dropout_rate=attention_dropout_rate,\n",
        "            key=cross_att_key,\n",
        "        )\n",
        "        self.ff_block = FeedForwardBlock(\n",
        "            d_model=d_model,\n",
        "            intermediate_size=intermediate_size,\n",
        "            dropout_rate=dropout_rate,\n",
        "            key=ff_key,\n",
        "        )\n",
        "        self.layer_norm_1 = eqx.nn.LayerNorm(shape=d_model)\n",
        "        self.layer_norm_2 = eqx.nn.LayerNorm(shape=d_model)\n",
        "        self.layer_norm_3 = eqx.nn.LayerNorm(shape=d_model)\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        inputs: Float[Array, \"tgt_seq_len d_model\"],\n",
        "        encoder_output,\n",
        "        src_pad_mask: Array = None,\n",
        "        tgt_pad_mask: Array = None,   # use ... (ellipsis) to indicate variable size in annotation - can't just leave square brackets empty\n",
        "        look_ahead_mask: Optional[bool] = False,\n",
        "        *,\n",
        "        enable_dropout: bool = False,\n",
        "        key: Optional[jr.PRNGKey] = None,\n",
        "    ):\n",
        "\n",
        "        self_attn_key, cross_attn_key, ff_key = (None, None, None) if key is None else jr.split(key, num=3)\n",
        "\n",
        "        inputs = inputs + self.self_att_block(\n",
        "            inputs, inputs, inputs, q_pad_mask=tgt_pad_mask, kv_pad_mask=tgt_pad_mask, look_ahead_mask=look_ahead_mask, enable_dropout=enable_dropout, key=self_attn_key\n",
        "        )\n",
        "        inputs = jax.vmap(self.layer_norm_1)(inputs)\n",
        "\n",
        "        inputs = inputs + self.cross_att_block(\n",
        "            inputs, encoder_output, encoder_output, q_pad_mask=tgt_pad_mask, kv_pad_mask=src_pad_mask, enable_dropout=enable_dropout, key=cross_attn_key\n",
        "        )\n",
        "        inputs = jax.vmap(self.layer_norm_2)(inputs)\n",
        "\n",
        "        inputs = inputs + self.ff_block(\n",
        "            inputs, enable_dropout=enable_dropout, key=ff_key\n",
        "        )\n",
        "        inputs = jax.vmap(self.layer_norm_3)(inputs)\n",
        "\n",
        "        return inputs"
      ],
      "metadata": {
        "id": "Uefs4CKaCod6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decoder**"
      ],
      "metadata": {
        "id": "nJ2NzZHgNxj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(eqx.Module):\n",
        "\n",
        "    embedder_block: InputEmbeddings\n",
        "    pos_embed: SinusoidalPosEmb\n",
        "    layers: List[DecoderLayer]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        d_model: int,\n",
        "        seq_len: int,\n",
        "        intermediate_size: int,\n",
        "        num_heads: int,\n",
        "        num_layers: int,\n",
        "        dropout_rate: float,\n",
        "        attention_dropout_rate: float,\n",
        "        key: jr.PRNGKey\n",
        "    ):\n",
        "        embedder_key, layer_key = jr.split(key, num=2)\n",
        "\n",
        "        self.embedder_block = InputEmbeddings(\n",
        "          d_model, vocab_size, embedder_key\n",
        "        )\n",
        "        self.pos_embed = SinusoidalPosEmb(d_model, seq_len, dropout_rate)\n",
        "        layer_keys = jr.split(layer_key, num=num_layers)\n",
        "\n",
        "        self.layers = [\n",
        "          DecoderLayer(\n",
        "              d_model=d_model, intermediate_size=intermediate_size, num_heads=num_heads, dropout_rate=dropout_rate, attention_dropout_rate=attention_dropout_rate, key=layer_key,\n",
        "          )\n",
        "          for layer_key in layer_keys]\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        x: Array,\n",
        "        encoder_output: Float[Array, \"src_seq_len d_model\"],\n",
        "        src_pad_mask: Array = None,\n",
        "        tgt_pad_mask: Array = None,\n",
        "        look_ahead_mask: Optional[bool] = False,\n",
        "        *,\n",
        "        enable_dropout: bool = False,\n",
        "        key: Optional[jr.PRNGKey] = None,\n",
        "    ):\n",
        "        print\n",
        "        ps_emb_key, l_key = (None, None) if key is None else jr.split(key, 2)\n",
        "\n",
        "        embed_inputs = self.embedder_block(x)\n",
        "        x = self.pos_embed(embed_inputs, key=ps_emb_key)\n",
        "        for layer in self.layers:\n",
        "              cl_key, l_key = (None, None) if l_key is None else jr.split(l_key)\n",
        "\n",
        "              x = layer(x, encoder_output, src_pad_mask=src_pad_mask, tgt_pad_mask=tgt_pad_mask, look_ahead_mask=look_ahead_mask, enable_dropout=enable_dropout, key=cl_key)\n",
        "\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "JmdNhQa5Nv9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final Layer**"
      ],
      "metadata": {
        "id": "GEM8TlL_RgQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Out_Projection_Layer(eqx.Module):\n",
        "\n",
        "    proj: eqx.nn.Linear\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model:int,\n",
        "        vocab_size: int,\n",
        "        key: jr.PRNGKey\n",
        "    ):\n",
        "        self.proj = eqx.nn.Linear(in_features=d_model, out_features=vocab_size, key=key)\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        x: Float[Array, \"tgt_seq_len d_model\"]\n",
        "    ) -> Int[Array, \"tgt_seq_len vocab_size\"]:\n",
        "        out = jax.vmap(self.proj)(x)\n",
        "\n",
        "        # return jax.nn.log_softmax(out, axis=-1) # - softmax included in CE loss\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "oajMZv-cRPtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transformer**"
      ],
      "metadata": {
        "id": "-j_Sa0VLTDwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(eqx.Module):\n",
        "\n",
        "    encoder: Encoder\n",
        "    decoder: Decoder\n",
        "    out_proj: Out_Projection_Layer\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        config: Mapping,\n",
        "        key: jr.PRNGKey\n",
        "    ):\n",
        "\n",
        "        encoder_key, decoder_key, out_proj_key = jr.split(key, num=3)\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            vocab_size=config[\"src_vocab_size\"],\n",
        "            d_model=config[\"d_model\"],\n",
        "            seq_len=config[\"src_seq_len\"],\n",
        "            intermediate_size=config[\"intermediate_size\"],\n",
        "            num_layers=config[\"num_hidden_layers\"],\n",
        "            num_heads=config[\"num_attention_heads\"],\n",
        "            dropout_rate=config[\"hidden_dropout_prob\"],\n",
        "            attention_dropout_rate=config[\"attention_dropout_prob\"],\n",
        "            key=encoder_key,\n",
        "        )\n",
        "        self.decoder = Decoder(\n",
        "            vocab_size=config[\"tgt_vocab_size\"],\n",
        "            d_model=config[\"d_model\"],\n",
        "            seq_len=config[\"tgt_seq_len\"],\n",
        "            intermediate_size=config[\"intermediate_size\"],\n",
        "            num_layers=config[\"num_hidden_layers\"],\n",
        "            num_heads=config[\"num_attention_heads\"],\n",
        "            dropout_rate=config[\"hidden_dropout_prob\"],\n",
        "            attention_dropout_rate=config[\"attention_dropout_prob\"],\n",
        "            key=decoder_key,\n",
        "        )\n",
        "        self.out_proj = Out_Projection_Layer(\n",
        "            d_model=config[\"d_model\"],\n",
        "            vocab_size=config[\"tgt_vocab_size\"],\n",
        "            key=out_proj_key)\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        src,\n",
        "        tgt,\n",
        "        src_pad_mask: Array = None,\n",
        "        tgt_pad_mask: Array = None,\n",
        "        look_ahead_mask: Array = None,\n",
        "        enable_dropout: bool = False,\n",
        "        key: Optional[jr.PRNGKey] = None\n",
        "    ):\n",
        "        enc_key, dec_key = (None, None) if key is None else jr.split(key, 2)\n",
        "\n",
        "        encoder_output = self.encoder(src, src_pad_mask, enable_dropout=enable_dropout, key=enc_key)\n",
        "        decoder_output = self.decoder(tgt, encoder_output, src_pad_mask, tgt_pad_mask, look_ahead_mask, enable_dropout=enable_dropout, key=dec_key)\n",
        "        output = self.out_proj(decoder_output)\n",
        "        return output\n",
        "\n",
        "    def encode(self,\n",
        "               src,\n",
        "               src_pad_mask: Array = None,\n",
        "               enable_dropout: bool = False,\n",
        "               key: Optional[jr.PRNGKey] = None\n",
        "    ):\n",
        "        return self.encoder(src, src_pad_mask, enable_dropout=enable_dropout, key=key)\n",
        "\n",
        "    def decode(self,\n",
        "               tgt,\n",
        "               enc_out,\n",
        "               src_pad_mask: Array = None,\n",
        "               tgt_pad_mask: Array = None,\n",
        "               look_ahead_mask:bool = True,\n",
        "               enable_dropout: bool = False,\n",
        "               key: Optional[jr.PRNGKey] = None\n",
        "    ):\n",
        "        return self.decoder(tgt, enc_out, src_pad_mask, tgt_pad_mask, look_ahead_mask, enable_dropout=enable_dropout, key=key)\n",
        "\n",
        "    def project(self, x):\n",
        "        return self.out_proj(x)"
      ],
      "metadata": {
        "id": "-z-D29cQd6EB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training**"
      ],
      "metadata": {
        "id": "CNe_Frjge_j7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Data utility functions"
      ],
      "metadata": {
        "id": "mU6I5N68sK5g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenizer**"
      ],
      "metadata": {
        "id": "34Ss-YPHrcha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_or_build_tokenizer(ds, lang):\n",
        "\n",
        "    def get_all_sentences(ds, lang):\n",
        "        for item in ds:\n",
        "            yield item['translation'][lang]\n",
        "\n",
        "    tokenizer_path = Path(\"tokenizer_{0}.json\".format(lang))\n",
        "\n",
        "    if not Path.exists(tokenizer_path):\n",
        "        tokenizer = Tokenizer(WordLevel(unk_token='[UNK]'))\n",
        "        tokenizer.pre_tokenizer = Whitespace()\n",
        "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\",\"[PAD]\",\"[SOS]\",\"[EOS]\"], min_frequency=2)\n",
        "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
        "        tokenizer.save(str(tokenizer_path))\n",
        "    else:\n",
        "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
        "    return tokenizer"
      ],
      "metadata": {
        "id": "64Ri2SWeWYxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train/Test split**"
      ],
      "metadata": {
        "id": "-KNj9vuzsO8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rand_split(ds,prop, key):\n",
        "    ds_size = len(ds[\"id\"])\n",
        "    train_size = int(prop * ds_size)\n",
        "    indices = jnp.arange(ds_size)\n",
        "    perm = jr.permutation(rand_key, indices, independent=False)  # get random permutation of indices\n",
        "    train_ind = perm[:train_size]\n",
        "    val_ind = perm[train_size:]\n",
        "    train_ds = datasets.Dataset.from_dict({\"id\":[ds[\"id\"][i] for i in train_ind], \"translation\":[ds[\"translation\"][i] for i in train_ind]})\n",
        "    val_ds = datasets.Dataset.from_dict({\"id\":[ds[\"id\"][i] for i in val_ind], \"translation\":[ds[\"translation\"][i] for i in val_ind]})\n",
        "    assert (len(train_ds[\"id\"]) + len(val_ds[\"id\"])) - ds_size == 0\n",
        "    assert (len(train_ds[\"translation\"]) + len(val_ds[\"translation\"])) - ds_size == 0\n",
        "\n",
        "    return (train_ds, val_ds)"
      ],
      "metadata": {
        "id": "eGGgZPtxp8us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get Dataset and train/test split**"
      ],
      "metadata": {
        "id": "NAklnFvz_0q9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ds(\n",
        "    gpt_config: dict,\n",
        "    split_key: jr.PRNGKey,\n",
        "    shorten: bool = True,\n",
        "    max_seq_len: int = None\n",
        "    ):\n",
        "\n",
        "\n",
        "    # load dataset #\n",
        "    ds_raw = load_dataset('opus_books', f'{gpt_config[\"src_lang\"]}-{gpt_config[\"tgt_lang\"]}', split='train')\n",
        "    ds_size = len(ds_raw[\"id\"])\n",
        "    assert ds_size == len(ds_raw[\"translation\"])\n",
        "\n",
        "    # tokenize input data #\n",
        "    tokenizer_src = get_or_build_tokenizer(ds_raw, gpt_config[\"src_lang\"])\n",
        "    tokenizer_tgt = get_or_build_tokenizer(ds_raw, gpt_config[\"tgt_lang\"])\n",
        "\n",
        "\n",
        "    max_len_src = 0\n",
        "    max_len_tgt = 0\n",
        "    for item in ds_raw:\n",
        "        src_ids = tokenizer_src.encode(item['translation'][gpt_config[\"src_lang\"]]).ids\n",
        "        tgt_ids = tokenizer_tgt.encode(item['translation'][gpt_config[\"tgt_lang\"]]).ids\n",
        "        max_len_src = max(max_len_src, len(src_ids))\n",
        "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
        "\n",
        "    print(f'Max length of source sentence: {max_len_src}')\n",
        "    print(f'Max length of target sentence: {max_len_tgt}')\n",
        "\n",
        "    if short == False:\n",
        "        max_seq_len\n",
        "\n",
        "    if shorten == True:\n",
        "        count=0\n",
        "        for item in ds_raw:\n",
        "            src_ids = tokenizer_src.encode(item['translation'][gpt_config[\"src_lang\"]]).ids\n",
        "            tgt_ids = tokenizer_tgt.encode(item['translation'][gpt_config[\"tgt_lang\"]]).ids\n",
        "            if len(src_ids) > max_seq_len or len(tgt_ids) > max_seq_len:\n",
        "                count+=1\n",
        "        print(f'Number of sentences longer than {max_seq_len} tokens: {count}')\n",
        "\n",
        "\n",
        "        # shorten training dataset #\n",
        "        short_ds_raw = {\"id\":[], \"translation\":[]}\n",
        "        for item in ds_raw:\n",
        "            src_ids = tokenizer_src.encode(item['translation'][gpt_config[\"src_lang\"]]).ids\n",
        "            tgt_ids = tokenizer_tgt.encode(item['translation'][gpt_config[\"tgt_lang\"]]).ids\n",
        "            if len(src_ids) <= max_seq_len and len(tgt_ids) <= max_seq_len:\n",
        "                short_ds_raw[\"id\"].append(item[\"id\"])\n",
        "                short_ds_raw[\"translation\"].append(item[\"translation\"])\n",
        "\n",
        "\n",
        "        max_len_src = 0\n",
        "        max_len_tgt = 0\n",
        "        for i in range(len(short_ds_raw[\"id\"])):\n",
        "            src_ids = tokenizer_src.encode(short_ds_raw['translation'][i][gpt_config[\"src_lang\"]]).ids\n",
        "            tgt_ids = tokenizer_tgt.encode(short_ds_raw['translation'][i][gpt_config[\"tgt_lang\"]]).ids\n",
        "            max_len_src = max(max_len_src, len(src_ids))\n",
        "            max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
        "\n",
        "        assert max_len_src == max_seq_len\n",
        "        assert max_len_tgt == max_seq_len\n",
        "\n",
        "        ds_size = len(short_ds_raw[\"id\"])\n",
        "        ds_raw = short_ds_raw\n",
        "\n",
        "    # train/validation split #\n",
        "    train_ds_raw, val_ds_raw = rand_split(ds_raw, gpt_config[\"train_val_split\"], split_key)   # defaults to 0.9 - 90% split of dataset\n",
        "\n",
        "    train_ds_size = len(train_ds_raw[\"id\"])\n",
        "    val_ds_size = len(val_ds_raw[\"id\"])\n",
        "\n",
        "    assert len(train_ds_raw[\"translation\"]) == train_ds_size and len(val_ds_raw[\"translation\"]) == val_ds_size\n",
        "    assert (train_ds_size + val_ds_size) == ds_size\n",
        "\n",
        "    if short==True:\n",
        "        src_seq_len = max_seq_len\n",
        "        tgt_seq_len = max_seq_len\n",
        "    else:\n",
        "        src_seq_len = max_len_src\n",
        "        tgt_seq_len = max_len_tgt\n",
        "\n",
        "    return train_ds_raw, val_ds_raw, tokenizer_src, tokenizer_tgt, ds_size, train_ds_size, val_ds_size, src_seq_len, tgt_seq_len"
      ],
      "metadata": {
        "id": "RKAeP-YG7ndS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data processing**"
      ],
      "metadata": {
        "id": "GzQnL7_baLXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslationDataset(data.Dataset):\n",
        "\n",
        "      def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, src_seq_len, tgt_seq_len):\n",
        "          super().__init__()\n",
        "\n",
        "\n",
        "          self.src_seq_len = src_seq_len\n",
        "          self.tgt_seq_len = tgt_seq_len\n",
        "\n",
        "          self.ds = ds\n",
        "          self.tokenizer_src = tokenizer_src\n",
        "          self.tokenizer_tgt = tokenizer_tgt\n",
        "          self.src_lang = src_lang\n",
        "          self.tgt_lang = tgt_lang\n",
        "\n",
        "          self.sos_token = np.asarray([tokenizer_src.token_to_id(\"[SOS]\")], dtype=int)\n",
        "          self.eos_token = np.asarray([tokenizer_src.token_to_id(\"[EOS]\")], dtype=int)\n",
        "          self.pad_token  = np.asarray([tokenizer_src.token_to_id(\"[PAD]\")], dtype=int)\n",
        "\n",
        "      def __len__(self):\n",
        "          return len(self.ds)\n",
        "\n",
        "      def __getitem__(self, idx):\n",
        "          src_target_pair = self.ds[idx]\n",
        "\n",
        "          src_text = src_target_pair['translation'][self.src_lang]\n",
        "          tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
        "\n",
        "          enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
        "          dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
        "\n",
        "\n",
        "          enc_num_padding_tokens = self.src_seq_len - len(enc_input_tokens) - 2\n",
        "          dec_num_padding_tokens = self.tgt_seq_len - len(dec_input_tokens) - 2   # should be -1 if not including EOS token\n",
        "          lab_num_padding_tokens = dec_num_padding_tokens + 1   # not necessary if omitting EOS token in decoder input\n",
        "\n",
        "\n",
        "          if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
        "              raise ValueError('Sentence is too long')\n",
        "\n",
        "          # add SOS and EOS to source text\n",
        "          encoder_input = np.concatenate([\n",
        "              self.sos_token,\n",
        "              np.asarray(enc_input_tokens),\n",
        "              self.eos_token,\n",
        "              np.repeat(self.pad_token, enc_num_padding_tokens)\n",
        "          ])\n",
        "\n",
        "\n",
        "          # add SOS to decoder input - I have also added EOS token - logically makes sense as we want the decoder to predict EOS at inference time! but I have seen implementations that specifically do not include EOS in decoder training input\n",
        "          decoder_input = np.concatenate([\n",
        "              self.sos_token,\n",
        "              np.asarray(dec_input_tokens, dtype=int),\n",
        "              self.eos_token,\n",
        "              np.repeat(self.pad_token, dec_num_padding_tokens)\n",
        "          ])\n",
        "\n",
        "          # add EOS to expected decoder output\n",
        "          label = np.concatenate([\n",
        "              np.asarray(dec_input_tokens, dtype=int),\n",
        "              self.eos_token,\n",
        "              np.repeat(self.pad_token, lab_num_padding_tokens) # not dec_num\n",
        "          ])\n",
        "\n",
        "          assert encoder_input.shape[0] == self.src_seq_len\n",
        "          assert decoder_input.shape[0] == self.tgt_seq_len\n",
        "          assert label.shape[0] == self.tgt_seq_len\n",
        "\n",
        "\n",
        "          src_mask = np.expand_dims((encoder_input!= self.pad_token), axis=0)  # (1, seq_len)\n",
        "          tgt_mask = np.expand_dims((decoder_input!= self.pad_token), axis=0) # & np.asarray(causal_attention_mask(decoder_input)) # (1 seq_len) & (seq_len seq_len)\n",
        "\n",
        "          return {\n",
        "              \"encoder_input\": encoder_input,\n",
        "              \"decoder_input\": decoder_input,\n",
        "              \"src_mask\": src_mask,\n",
        "              \"tgt_mask\": tgt_mask,\n",
        "              \"label\": label,\n",
        "              \"src_text\": src_text,\n",
        "              \"tgt_text\": tgt_text,\n",
        "          }\n"
      ],
      "metadata": {
        "id": "46vKZ1kQzCFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Inference functions"
      ],
      "metadata": {
        "id": "YjlDijTsETux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Greedy decoding function**"
      ],
      "metadata": {
        "id": "8AfD212FAc_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_decode(\n",
        "    model,\n",
        "    tokenizer_src,\n",
        "    tokenizer_tgt,\n",
        "    max_len,\n",
        "    src,\n",
        "    src_mask\n",
        "):\n",
        "    @eqx.filter_jit\n",
        "    def decode_pass(model, encoder_output,src_mask, decoder_input):\n",
        "        out = model.decode(decoder_input, encoder_output, src_mask, look_ahead_mask=True)\n",
        "        prob = jax.nn.softmax(model.project(out), axis=-1)\n",
        "        next_word = jnp.argmax(prob[-1])\n",
        "        return next_word\n",
        "\n",
        "    sos_idx = tokenizer_tgt.token_to_id(\"[SOS]\")\n",
        "    eos_idx = tokenizer_tgt.token_to_id(\"[EOS]\")\n",
        "    encoder_output = model.encode(src, src_mask)   # This computation now gets used repeatedly\n",
        "    dec_pass = functools.partial(decode_pass, model, encoder_output, src_mask)\n",
        "    decoder_input = jnp.asarray([sos_idx])    # SOS token to start decoding\n",
        "    for i in range(max_len):\n",
        "        dec_shape = decoder_input.shape[-1]\n",
        "        next_word = dec_pass(decoder_input)\n",
        "        if next_word == eos_idx:\n",
        "            break\n",
        "        decoder_input = jnp.concatenate([decoder_input, jnp.asarray([next_word])])\n",
        "\n",
        "    return decoder_input"
      ],
      "metadata": {
        "id": "dCjSlQk7LIJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Validation/Accuracy function**"
      ],
      "metadata": {
        "id": "n0olfN6YJoR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(model, input, tokenizer_tgt, max_len):\n",
        "    inf_model = eqx.tree_inference(model, value=True)\n",
        "    encoder_input = jnp.squeeze(jnp.asarray(input[\"encoder_input\"], dtype=int), axis = 0)\n",
        "    src_mask = jnp.squeeze(jnp.asarray(input[\"src_mask\"]), axis = 0)\n",
        "\n",
        "    model_output = greedy_decode(inf_model, tokenizer_src, tokenizer_tgt, max_len, encoder_input, src_mask)\n",
        "\n",
        "    predicted = str(tokenizer_tgt.decode(model_output))\n",
        "    expected = input[\"tgt_text\"][0]\n",
        "\n",
        "    #print(predicted, expected)\n",
        "    metric1 = WordErrorRate()\n",
        "    lossWER = metric1(predicted, expected)\n",
        "\n",
        "    metric2 = CharErrorRate()\n",
        "    lossCER = metric2(predicted, expected)\n",
        "\n",
        "    metric3 = BLEUScore()\n",
        "    lossBS = metric3(predicted, expected)\n",
        "\n",
        "    return float(lossWER), float(lossCER), float(lossBS)"
      ],
      "metadata": {
        "id": "i1UISk6KZHsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Training functions"
      ],
      "metadata": {
        "id": "KlRQZ2SeEYoR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss function** - *with label smoothing*"
      ],
      "metadata": {
        "id": "jqAMKdJrGKlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(model, encoder_inputs, src_masks, decoder_inputs, tgt_masks, labels, key):   # key\n",
        "    batch_size = gpt_config[\"batch_size\"]    # encoder_inputs.shape[0]\n",
        "    batch_keys = jr.split(key, num=batch_size)\n",
        "\n",
        "    model_output = jax.vmap(model, in_axes=(0, 0, 0, 0, None, None, 0))(encoder_inputs, decoder_inputs, src_masks, tgt_masks, True, True, batch_keys)\n",
        "\n",
        "    output = einops.rearrange(model_output, \"b s v -> (b s) v\") # , b=batch_size, s = proj_output[1], tokenizer_tgt.get_vocab_size())\n",
        "    labels = einops.rearrange(labels, \" b s -> (b s)\")\n",
        "\n",
        "    pad_pos = tokenizer_tgt.token_to_id(\"[PAD]\")\n",
        "\n",
        "    oh_labels = jax.nn.one_hot(labels, tokenizer_tgt.get_vocab_size())\n",
        "    oh_labels = oh_labels.at[:, pad_pos].set(0.)  # unnecessary step but useful if I want to get rid of \"label smoothing\"\n",
        "\n",
        "    s_labels = optax.smooth_labels(oh_labels, alpha=0.1)  # label smoothing\n",
        "\n",
        "    s_labels = s_labels.at[:, pad_pos].set(0.)   # set all \"vertical\" padding tokens to one i.e. all possible alternatives to 0 (this wouldn't be necessary with no label smoothing - redundacy yet)\n",
        "    mask = jnp.asarray(labels == pad_pos) # T/F - true/false mask\n",
        "    new_labels = jax.vmap(jnp.where, in_axes=(0,None, 0))(mask, 0., s_labels)  # so padding token is zero vertically but also entire rows of the paddding tokens are zero (think cross in a way)\n",
        "    pad_mask = jnp.where(labels==pad_pos, 0., 1.)  # 1/0  - useful for working out number of non-padding tokens\n",
        "\n",
        "    output=output.at[:, pad_pos].set(-1e4)  #or -jnp.inf - for model output, ensure for every prediction that the padding token is set as negative possible for getting zero probability in the softmax\n",
        "\n",
        "    loss = optax.softmax_cross_entropy(logits=output, labels=new_labels)  #compute cross entropy loss\n",
        "\n",
        "    return jnp.sum(loss)/jnp.sum(pad_mask)  # calculate mean of non padding tokens only!"
      ],
      "metadata": {
        "id": "I_gQhlHJF_4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Update step**"
      ],
      "metadata": {
        "id": "tN-nLM71GIbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@eqx.filter_jit\n",
        "def make_step(model, encoder_inputs, encoder_masks, decoder_inputs, decoder_masks, labels, opt_state, opt_update, key):  # key\n",
        "    key, new_key = jr.split(key)\n",
        "    loss_fn = eqx.filter_value_and_grad(compute_loss)\n",
        "    loss, grads = loss_fn(model, encoder_inputs, encoder_masks, decoder_inputs, decoder_masks, labels, key) # key\n",
        "    updates, opt_state = opt_update(grads, opt_state)\n",
        "    model = eqx.apply_updates(model, updates)\n",
        "\n",
        "    return loss, model, opt_state, new_key"
      ],
      "metadata": {
        "id": "K3mMjBMJGIbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model initialisation** - *Including custom Xavier uniform initialisation*"
      ],
      "metadata": {
        "id": "LdyOcw7xGIbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# weight initialiser #\n",
        "def xavier_init(param: jax.Array, key = jr.PRNGKey) -> jax.Array:\n",
        "    initialiser = jax.nn.initializers.glorot_uniform()\n",
        "    params = initialiser(key, param.shape)\n",
        "    return params\n",
        "\n",
        "# bias initialiser #\n",
        "def xavier_init_b(param: jax.Array, key = jr.PRNGKey) -> jax.Array:\n",
        "    initialiser = jax.nn.initializers.glorot_uniform()\n",
        "    params = initialiser(key, (1, param.shape[0]))\n",
        "    params = jnp.squeeze(params, axis=0)\n",
        "    return params\n",
        "\n",
        "# initiliase custom model parameters #\n",
        "def init_parameters(model, init_fn, init_fn_b, key):\n",
        "\n",
        "    wkey, bkey, ekey = jr.split(key, 3)\n",
        "\n",
        "    is_linear = lambda x: isinstance(x, eqx.nn.Linear)\n",
        "\n",
        "    get_weights = lambda m: [x.weight for x in jax.tree_util.tree_leaves(m, is_leaf=is_linear) if is_linear(x)]\n",
        "    weights = get_weights(model)\n",
        "    new_weights = [init_fn(weight, subkey) for weight, subkey in zip(weights, jr.split(wkey, len(weights)))]\n",
        "    model_w = eqx.tree_at(get_weights, model, new_weights)\n",
        "\n",
        "    get_biases = lambda m: [x.bias for x in jax.tree_util.tree_leaves(m, is_leaf=is_linear) if is_linear(x)]\n",
        "    biases = get_biases(model_w)\n",
        "    new_biases = [init_fn_b(bias, subkey) for bias, subkey in zip(biases, jr.split(bkey, len(biases)))]\n",
        "    model_wb = eqx.tree_at(get_biases, model_w, new_biases)\n",
        "\n",
        "    is_embedding = lambda x: isinstance(x, eqx.nn.Embedding)\n",
        "\n",
        "    get_embs = lambda m: [x.weight for x in jax.tree_util.tree_leaves(m, is_leaf=is_embedding) if is_embedding(x)]\n",
        "    embs = get_embs(model_wb)\n",
        "    new_embs = [init_fn(embedding, subkey) for embedding, subkey in zip(embs, jr.split(ekey, len(embs)))]\n",
        "    new_model = eqx.tree_at(get_embs, model_wb, new_embs)\n",
        "\n",
        "    return new_model"
      ],
      "metadata": {
        "id": "ZWJgnyFlGIbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def xav_init_model(gpt_config: dict, r_init_key: jr.PRNGKey, c_init_key: jr.PRNGKey)\n",
        "    # custom model initialisation #\n",
        "    init_model = Transformer(gpt_config, r_init_key)\n",
        "    model = init_parameters(init_model, xavier_init, xavier_init_b, c_init_key)\n",
        "    return model"
      ],
      "metadata": {
        "id": "_zX59VhvAt32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Training Loop"
      ],
      "metadata": {
        "id": "L0ANI1jGEcWh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train model**"
      ],
      "metadata": {
        "id": "DAH_1fvI_6-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_transformer(\n",
        "    d_model: int = 512,\n",
        "    num_hidden_layers: int = 6,\n",
        "    num_attention_heads: int = 8,\n",
        "    intermediate_size: int = 2048,\n",
        "    src_lang: str = \"en\",\n",
        "    tgt_lang: str = \"fr\",\n",
        "    epochs: int = 10,\n",
        "    batch_size: int = 32,\n",
        "    learning_rate: float = 1e-4,\n",
        "    train_val_split: float = 0.9,\n",
        "    short: bool: False,\n",
        "    max_seq_len: int = None,\n",
        "    save: bool = False):\n",
        "\n",
        "    gpt_config = {\n",
        "        \"src_vocab_size\": int,\n",
        "        \"tgt_vocab_size\": int,\n",
        "        \"src_seq_len\": int,\n",
        "        \"tgt_seq_len\": int,\n",
        "        \"d_model\": d_model,\n",
        "        \"num_hidden_layers\": num_hidden_layers,\n",
        "        \"num_attention_heads\": num_attention_heads,\n",
        "        \"intermediate_size\": intermediate_size,\n",
        "        \"hidden_dropout_prob\": 0.1,\n",
        "        \"attention_dropout_prob\": 0.1,\n",
        "        \"src_lang\": src_lang,\n",
        "        \"tgt_lang\": tgt_lang,\n",
        "        \"epochs\": epochs,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"train_ds_size\": int,\n",
        "        \"val_batch_size\":1,\n",
        "        \"train_val_split\": train_val_split,\n",
        "    }\n",
        "\n",
        "    train_key, loader_key, split_key, r_init_key, c_init_key, val_key = jr.split(rand_key, 6)  #check right no. keys!!!!\n",
        "    train_ds_raw, val_ds_raw, tokenizer_src, tokenizer_tgt, ds_size, train_ds_size, val_ds_size, src_seq_len, tgt_seq_len  = get_ds(gpt_config, split_key, short=True, max_seq_len)\n",
        "\n",
        "\n",
        "\n",
        "    # update gpt configuration #\n",
        "    new_config = {\n",
        "        \"src_vocab_size\": tokenizer_src.get_vocab_size(),\n",
        "        \"tgt_vocab_size\": tokenizer_tgt.get_vocab_size(),\n",
        "        \"src_seq_len\": src_seq_len + 5,\n",
        "        \"tgt_seq_len\": tgt_seq_len + 5,\n",
        "        \"train_ds_size\": train_ds_size\n",
        "        }\n",
        "    gpt_config.update(new_config)\n",
        "\n",
        "\n",
        "    train_ds = TranslationDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, gpt_config[\"src_lang\"], gpt_config[\"tgt_lang\"], gpt_config[\"src_seq_len\"], gpt_config[\"tgt_seq_len\"])\n",
        "    val_ds = TranslationDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, gpt_config[\"src_lang\"], gpt_config[\"tgt_lang\"], gpt_config[\"src_seq_len\"], gpt_config[\"tgt_seq_len\"])\n",
        "\n",
        "    train_dataloader = data.DataLoader(train_ds, batch_size=gpt_config[\"batch_size\"], shuffle=True, drop_last=True)\n",
        "    val_dataloader = data.DataLoader(val_ds, batch_size=gpt_config[\"val_batch_size\"], shuffle=True, drop_last=True)\n",
        "\n",
        "    model = xav_init_model(gpt_config, r_init_key, c_init_key)\n",
        "\n",
        "    # setup optax #\n",
        "    opt = optax.inject_hyperparams(optax.adam)(learning_rate = optax.warmup_cosine_decay_schedule(\n",
        "        init_value=1e-9,\n",
        "        peak_value=1e-3,\n",
        "        warmup_steps = math.floor((gpt_config[\"train_ds_size\"]//gpt_config[\"batch_size\"]) * gpt_config[\"epochs\"] * 0.075),\n",
        "        decay_steps = math.floor((gpt_config[\"train_ds_size\"]//gpt_config[\"batch_size\"]) * gpt_config[\"epochs\"] * 0.925),\n",
        "        end_value = 2e-4)  # min value (alpha) for cosine schedule is alpha = end_value / peak_value - lr will stay constant after reaching this scalar value\n",
        "    )\n",
        "\n",
        "    # opt = optax.adam(learning_rate=gpt_config[\"learning_rate\"])\n",
        "\n",
        "    opt = optax.chain(optax.clip_by_global_norm(1.0), opt)\n",
        "\n",
        "    opt_state = opt.init(eqx.filter(model, eqx.is_inexact_array))\n",
        "\n",
        "    # training loop #\n",
        "    train_loss = {}   # for graphing training loss and validation loss\n",
        "    val_loss = {}\n",
        "    epochs = gpt_config[\"epochs\"]\n",
        "    ocr = 0.2   # % of steps that we want to validate the model\n",
        "    num_steps = math.floor(gpt_config[\"train_ds_size\"]//gpt_config[\"batch_size\"] * ocr)\n",
        "    curr_state = 0 # this is the state pertaining to validation\n",
        "    v_ds = iter(val_dataloader)\n",
        "    for epoch in range(epochs):\n",
        "        with tqdm.tqdm(\n",
        "            train_dataloader,\n",
        "            total = gpt_config[\"train_ds_size\"]//gpt_config[\"batch_size\"],\n",
        "            unit = \"steps\",\n",
        "            desc = f\"Epoch {epoch+1}/{epochs}\",\n",
        "        ) as tqdm_epoch:\n",
        "            for data in tqdm_epoch:\n",
        "\n",
        "                encoder_input = jnp.asarray(data[\"encoder_input\"], dtype=int)\n",
        "                src_mask = jnp.asarray(data[\"src_mask\"])\n",
        "                decoder_input = jnp.asarray(data[\"decoder_input\"], dtype=int)\n",
        "                tgt_mask = jnp.asarray(data[\"tgt_mask\"])\n",
        "                label = jnp.asarray(data[\"label\"])\n",
        "\n",
        "\n",
        "                tr_loss, model, opt_state, train_key = make_step(model, encoder_input, src_mask, decoder_input, tgt_mask, label, opt_state, opt.update, train_key)\n",
        "\n",
        "                if curr_state % num_steps == 0:\n",
        "                    w_l, c_l, BS_l = accuracy(model, next(v_ds), tokenizer_tgt, max_len = gpt_config[\"tgt_seq_len\"])\n",
        "\n",
        "\n",
        "                tqdm_epoch.set_postfix(loss = tr_loss, word_error=w_l, character_error=c_l, BLEUScore=BS_l)\n",
        "                curr_state += 1\n",
        "\n",
        "            loader_key = jr.split(loader_key, 1)[0]\n",
        "            train_loss[epoch]=tr_loss\n",
        "            val_loss[epoch]=w_l\n",
        "\n",
        "    if save == True:\n",
        "        # save model #\n",
        "        eqx.tree_serialise_leaves(\"EnFr2nd.eqx\", model)\n",
        "        shutil.copy('/content/EnFr2nd.eqx','/content/gdrive/MyDrive/Colab_Notebooks')\n",
        "\n",
        "        # save train and validation loss #\n",
        "        with open('/content/EnFr2nd_train_loss.pkl', 'wb') as file:\n",
        "            dump(train_loss, file)\n",
        "        shutil.copy('/content/EnFr2nd_train_loss.pkl','/content/gdrive/MyDrive/Colab_Notebooks')\n",
        "\n",
        "        with open('/content/EnFr2nd_val_loss.pkl', 'wb') as file:\n",
        "            dump(train_loss, file)\n",
        "        shutil.copy('/content/EnFr2nd_val_loss.pkl','/content/gdrive/MyDrive/Colab_Notebooks')\n",
        "\n",
        "\n",
        "    return model, train_loss, val_loss"
      ],
      "metadata": {
        "id": "yvz0i8LV_bzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test saving works #\n",
        "empty_array = jnp.array(())\n",
        "with open('./pls_del.pkl', 'wb') as file:\n",
        "    dump(empty_array, file)\n",
        "shutil.copy('/content/pls_del.pkl','/content/gdrive/MyDrive/Colab_Notebooks')"
      ],
      "metadata": {
        "id": "5vAAB194Go-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train model #\n",
        "model, train_loss, val_loss = train_transformer()"
      ],
      "metadata": {
        "id": "zQr8TQ-3EhOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Misc"
      ],
      "metadata": {
        "id": "9-eXriEyEpFr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load saved model**"
      ],
      "metadata": {
        "id": "1NOYXoolD46R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer(gpt_config, r_init_key)\n",
        "loaded_model = eqx.tree_deserialise_leaves('/content/gdrive/MyDrive/Colab_Notebooks/EnFrVanilla.eqx', model)"
      ],
      "metadata": {
        "id": "AmqC789RkrcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load train or validation loss data**"
      ],
      "metadata": {
        "id": "PEQnc_4nkfWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# = load(open('gdrive/MyDrive/Colab_Notebooks/\"...\" ', 'rb')) - insert file name in ellipsis location"
      ],
      "metadata": {
        "id": "G5uOTYvisuFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plot loss curve**"
      ],
      "metadata": {
        "id": "VlFLyH9Mjuxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epcs = jnp.arange(1, epochs+1, step=1)\n",
        "plt.plot(epcs, train_loss.values(), label='Training loss')\n",
        "plt.plot(epcs, val_loss.values(), label='Validation loss')\n",
        "\n",
        "plt.title('Training Loss and Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.xticks(jnp.arange(0, epochs+1, steps=2))\n",
        "\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "gJcfYzdvjyCt",
        "outputId": "d345d80a-9778-4cb4-8ce4-9d95c1dcefcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Translate custom string**"
      ],
      "metadata": {
        "id": "nSWMTylqD_M_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(string: str):\n",
        "\n",
        "    #string = str(string)\n",
        "\n",
        "    inference_model = eqx.tree_inference(loaded_model, value=True)\n",
        "\n",
        "    sos_token = jnp.asarray([tokenizer_src.token_to_id(\"[SOS]\")], dtype=int)\n",
        "    eos_token = jnp.asarray([tokenizer_src.token_to_id(\"[EOS]\")], dtype=int)\n",
        "    pad_token  = jnp.asarray([tokenizer_src.token_to_id(\"[PAD]\")], dtype=int)\n",
        "\n",
        "    enc_input_tokens = tokenizer_src.encode(string).ids\n",
        "\n",
        "    enc_num_padding_tokens = gpt_config[\"src_seq_len\"] - len(enc_input_tokens) - 2\n",
        "\n",
        "    if enc_num_padding_tokens < 0:\n",
        "        raise ValueError('Sentence is too long')\n",
        "\n",
        "\n",
        "    encoder_input = jnp.concatenate([\n",
        "        sos_token,\n",
        "        jnp.asarray(enc_input_tokens),\n",
        "        eos_token,\n",
        "        jnp.repeat(pad_token, enc_num_padding_tokens)\n",
        "    ])\n",
        "\n",
        "    assert encoder_input.shape[0] == gpt_config[\"src_seq_len\"]\n",
        "    encoder_mask = jnp.expand_dims((encoder_input!= pad_token), axis=0)\n",
        "\n",
        "    encoder_input = jnp.asarray(encoder_input)\n",
        "    encoder_mask = jnp.asarray(encoder_mask)\n",
        "    model_output = greedy_decode(inference_model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len=50)\n",
        "\n",
        "    print(f'PREDICTED: {tokenizer_tgt.decode(model_output)}')"
      ],
      "metadata": {
        "id": "edaU7ExhICUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate('I will go home tomorrow, but I think I will arrive late.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rS1mfqS630sG",
        "outputId": "b1d81a7e-b5f5-413e-957f-067578171988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PREDICTED: Je rentre chez lui , mais je crois que je arrivera tard .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention Visualisation"
      ],
      "metadata": {
        "id": "AnvBAS6y90Jm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hs0v8JZc_Vjh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}