{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNsaBrvIfODqLhAPgyJXq4C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/durml91/Personal/blob/main/Transformer_implementation_I.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops\n",
        "!pip install equinox"
      ],
      "metadata": {
        "id": "Or-KppZLpwZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U jax jaxlib"
      ],
      "metadata": {
        "id": "yIDvm_6XqTu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IgYFrPvSpbjk"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.random as jr\n",
        "import jax.numpy as jnp\n",
        "import einops\n",
        "import equinox as eqx\n",
        "import optax\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "from typing import Dict, List, Mapping, Optional, Callable\n",
        "\n",
        "#from datasets import load_dataset\n",
        "\n",
        "from jaxtyping import Array, Float, Int\n",
        "\n",
        "from tqdm import notebook as tqdm\n",
        "\n",
        "import math"
      ],
      "metadata": {
        "id": "QEVh-COM17CC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key = jr.PRNGKey(2022)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52URpWXEprjm",
        "outputId": "989a0a0c-bbe1-497c-e0be-f36650aee5af"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer modules"
      ],
      "metadata": {
        "id": "P6oW6sooqpX1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GELU"
      ],
      "metadata": {
        "id": "zokOJ5im3RfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Lambda(eqx.Module):\n",
        "\n",
        "    fn: Callable\n",
        "\n",
        "    def __call__(self, x, *, key=None):\n",
        "        return self.fn(x)"
      ],
      "metadata": {
        "id": "kKw12VA93Q1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention"
      ],
      "metadata": {
        "id": "ZX585wtq5F0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionBlock(eqx.Module):\n",
        "\n",
        "    attention: eqx.nn.MultiheadAttention\n",
        "    layernorm: eqx.nn.LayerNorm\n",
        "    dropout: eqx.nn.Dropout\n",
        "    num_heads: int = eqx.field(static=True)\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size: int,\n",
        "        num_heads: int,\n",
        "        dropout_rate: float,\n",
        "        attention_dropout_rate: float,\n",
        "        key: jr.PRNGKey\n",
        "    ):\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = eqx.nn.MultiheadAttention(\n",
        "            num_heads=num_heads,\n",
        "            query_size=hidden_size,\n",
        "            use_query_bias=True,\n",
        "            use_key_bias=True,\n",
        "            use_value_bias=True,\n",
        "            use_output_bias=True,\n",
        "            dropout_p=attention_dropout_rate,\n",
        "            key=key,\n",
        "        )\n",
        "\n",
        "        self.layernorm = eqx.nn.LayerNorm(shape=hidden_size)\n",
        "        self.dropout = eqx.nn.Dropout(dropout_rate)\n",
        "\n",
        "\n",
        "    def make_self_attention_mask(\n",
        "        self, mask: Int[Array, \"seq_len\"]\n",
        "    ) -> Float[Array, \"num_heads seq_len seq_len\"]:\n",
        "\n",
        "        mask = jnp.multiply(\n",
        "            jnp.expand_dims(mask, axis=-1), jnp.expand_dims(mask, axis=-2)\n",
        "        )\n",
        "\n",
        "        #see if you can do this with einops rearrange or repeat (repeat allows you to add any number of dimensions in new axis)\n",
        "\n",
        "        mask = jnp.expand_dims(mask, axis=-3)\n",
        "        mask = jnp.repeat(mask, repeats=self.num_heads, axis=-3)\n",
        "\n",
        "        return mask.astype(jnp.float32)\n",
        "\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        input1: Float[Array, \"seq_len hidden_size\"],\n",
        "        input2: Float[Array, \"seq_len hidden_size\"],\n",
        "        input3: Float[Array, \"seq_len hidden_size\"],\n",
        "        mask: Optional[Int[Array, \"seq_len\"]],\n",
        "        enable_dropout: bool = False,\n",
        "        key: \"jr.PRNGKey\" = None,\n",
        "    ) -> Float[Array, \"seq_len hidden_size\"]:\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = self.make_self_attention_mask(mask)\n",
        "\n",
        "        attention_key, dropout_key = (\n",
        "            (None, None) if key is None else jr.split(key)\n",
        "        )\n",
        "\n",
        "        attention_output = self.attention(\n",
        "              query=input1,\n",
        "              key_=input2,\n",
        "              value=input3,\n",
        "              mask=mask,\n",
        "              inference=not enable_dropout,\n",
        "              key=attention_key\n",
        "        )\n",
        "\n",
        "        att_drop = self.dropout(attention_output, inference=not enable_dropout, key=dropout_key)\n",
        "        unn_out = att_drop + inputs\n",
        "        output = jax.vmap(self.layernorm)(unn_out)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "0PShmTm65HGC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP Block"
      ],
      "metadata": {
        "id": "BsMLjw_gEfsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardBlock(eqx.Module):\n",
        "\n",
        "    mlp: eqx.nn.Sequential    #could also use MLP if this way is more fiddly\n",
        "\n",
        "    layernorm: eqx.nn.LayerNorm\n",
        "    dropout: eqx.nn.Dropout\n",
        "\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size: int,\n",
        "        intermediate_size: int,\n",
        "        dropout_rate: float,\n",
        "        key: jr.PRNGKey,\n",
        "    ):\n",
        "\n",
        "        linear1, linear2 = jr.split(key)\n",
        "\n",
        "        self.mlp = eqx.nn.Sequential([\n",
        "            eqx.nn.Linear(in_features=intermediate_size, out_features=intermediate_size, key=linear1),\n",
        "            Lambda(jax.nn.gelu),\n",
        "            eqx.nn.Linear(in_features=intermediate_size, out_features=hidden_size, key=linear2)\n",
        "        ])\n",
        "\n",
        "        self.layernorm = eqx.nn.LayerNorm(shape=hidden_size)\n",
        "        self.dropout = eqx.nn.Dropout(dropout_rate)\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        inputs: Float[Array, \"seq_len hidden_size\"],\n",
        "        enable_dropout: bool = True,\n",
        "        key: Optional[jr.PRNGKey] = None,\n",
        "    ) -> Float[Array, \"seq_len hidden_size\"]:\n",
        "\n",
        "        feed_out = self.mlp(inputs)\n",
        "\n",
        "        out_d = self.dropout(feed_out, inference=not enable_dropout, key=key)\n",
        "\n",
        "        out_unn = out_d + inputs\n",
        "\n",
        "        output = self.layernorm(out_unn)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "Npn_D1FBEOTi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding table"
      ],
      "metadata": {
        "id": "zvVB7q4OSMvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InputEmbeddings(eqx.Module):\n",
        "\n",
        "  embedding: eqx.nn.Embedding\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      d_model: int,\n",
        "      vocab_size: int,\n",
        "      key: jr.PRNGKey,\n",
        "  ):\n",
        "\n",
        "      self.d_model = d_model\n",
        "      self.vocab_size = vocab_size\n",
        "      self.embedding = eqx.nn.Embedding(vocab_size, d_model, key)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return self.embedding(x) * math.sqrt(self.d_model)"
      ],
      "metadata": {
        "id": "gW98v6F7qjxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positional encoding"
      ],
      "metadata": {
        "id": "eGzfSNHsSOb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SinusoidalPosEmb(eqx.Module):\n",
        "    pos_emb: jax.Array\n",
        "    dropout: eqx.nn.Dropout\n",
        "\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        seq_len: int,\n",
        "        dropout_rate: float\n",
        "    ):\n",
        "        self.dropout = eqx.nn.Dropout(dropout_rate)\n",
        "\n",
        "        pe = jnp.zeros((seq_len, d_model))\n",
        "        position = jnp.expand_dims(jnp.arange(0 , seq_len), axis=1)\n",
        "        div_term - jnp.exp(jnp.arange(0, d_model, 2) * -(math.log(10_000) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = jnp.sin(position * div_term)\n",
        "        pe[:, 1::2] = jnp.cos(position * div_term)\n",
        "\n",
        "        self.pos_emb = jnp.expand_dims(pe, axis=0)\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        x,\n",
        "        enable_dropout: bool = False,\n",
        "        key: \"jr.PRNGKey\" = None,\n",
        "    ) -> Float[Array, \"seq_len d_model\"]:\n",
        "\n",
        "        x = x + self.pos_emb[:, :x.size(1)]\n",
        "\n",
        "        return self.dropout(x, inference=not enable_dropout, key=key)\n",
        "\n",
        "\n",
        "\n",
        "####### need to freeze parameters use eqx.partition or try filter(static=True)"
      ],
      "metadata": {
        "id": "Ua8tZGvG7pe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer Block"
      ],
      "metadata": {
        "id": "kPL-N55ntQmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerLayer(eqx.Module):\n",
        "\n",
        "    attention_block: AttentionBlock\n",
        "    ff_block: FeedForwardBlock\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size: int,\n",
        "        intermediate_size: int,\n",
        "        num_heads: int,\n",
        "        dropout_rate: float,\n",
        "        attention_dropout_rate: float,\n",
        "        key: jr.PRNGKey,\n",
        "    ):\n",
        "\n",
        "        attention_key, ff_key = jr.split(key)\n",
        "\n",
        "        self.attention_block = AttentionBlock(\n",
        "            hidden_size=hidden_size,\n",
        "            num_heads=num_heads,\n",
        "            dropout_rate=dropout_rate,\n",
        "            attention_dropout_rate=attention_dropout_rate,\n",
        "            key=attention_key,\n",
        "        )\n",
        "\n",
        "        self.ff_block = FeedForwardBlock(\n",
        "            hidden_size=hidden_size,\n",
        "            intermediate_size=intermediate_size,\n",
        "            dropout_rate=dropout_rate,\n",
        "            key=ff_key,\n",
        "        )\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        inputs: Float[Array, \"seq_len hidden_size\"],\n",
        "        mask: Optional[Int[Array, \"seq_len\"]] = None,\n",
        "        *,\n",
        "        enable_dropout: bool = False,\n",
        "        key: Optional[jr.PRNGKey] = None,\n",
        "    ) -> Float[Array, \"seq_len hidden_size\"]:\n",
        "\n",
        "        attn_key, ff_key = (None, None) if key is None else jr.split(key)\n",
        "\n",
        "        attention_output = self.attention_block(\n",
        "            inputs, inputs, inputs, mask, enable_dropout=enable_dropout, key=attn_key\n",
        "        )\n",
        "\n",
        "        mlp_out = self.ff_block(\n",
        "            inputs, enable_dropout=enable_dropout, ff_key\n",
        "        )\n",
        "\n",
        "        return mlp_out"
      ],
      "metadata": {
        "id": "s_zpRdXMtP9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder"
      ],
      "metadata": {
        "id": "7ZDEK-86ym-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(eqx.Module):\n",
        "\n",
        "    embedder_block: InputEmbeddings\n",
        "    pos_embed: SinusoidalPosEmb\n",
        "    layers: List[TransformerLayer]\n",
        "\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        hidden_size: int,\n",
        "        seq_len: int,\n",
        "        intermediate_size: int,\n",
        "        num_layers: int,\n",
        "        num_heads: int,\n",
        "        dropout_rate: float,\n",
        "        attention_dropout_rate: float,\n",
        "        key: jr.PRNGKey,\n",
        "    ):\n",
        "\n",
        "      embedder_key, layer_key = jr.split(key, num=2)\n",
        "\n",
        "      self.embedder_block = InputEmeddings(\n",
        "          hidden_size, vocab_size, embedder_key\n",
        "      )\n",
        "\n",
        "      self.pos_embed = SinusoidalPosEmb(hidden_size, seq_len, dropout_rate)\n",
        "\n",
        "      layer_keys = jr.split(layer_key, num=num_layers)\n",
        "\n",
        "      self.layers = [\n",
        "          TransformerLayer(\n",
        "              hidden_size=hidden_size, intermediate_size=intermediate_size, num_heads=num_heads, dropout_rate=dropout_rate, attention_dropout_rate=attention_dropout_rate, key=layer_key,\n",
        "          )\n",
        "          for layer_key in layer_keys]\n",
        "\n",
        "\n",
        "\n",
        "      def __call__(\n",
        "          self,\n",
        "          tokens: Int[Array, \" seq_len\"],\n",
        "          mask,\n",
        "          *,\n",
        "          enable_dropout: bool = False,\n",
        "          key: Optional[jr.PRNGKey] = None,\n",
        "      ):\n",
        "\n",
        "          embed_inputs = self.embedder_block(tokens)\n",
        "          x = self.pos_embed(embed_inputs, key=key)\n",
        "\n",
        "          for layer in self.layers:\n",
        "\n",
        "              x = layer(x, mask)\n",
        "\n",
        "          return x"
      ],
      "metadata": {
        "id": "3Szh7x1sylML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoder Block"
      ],
      "metadata": {
        "id": "7Ayup6V6FxC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(eqx.Module):\n",
        "\n",
        "    self_att_block: AttentionBlock\n",
        "    cross_att_block: AttentionBlock\n",
        "    ff_block: FeedForwardBlock\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size: int,\n",
        "        intermediate_size: int,\n",
        "        num_heads: int,\n",
        "        dropout_rate: float,\n",
        "        attention_dropout_rate: float,\n",
        "        key: jr.PRNGKey,\n",
        "    ):\n",
        "\n",
        "       self_att_key, cross_att_key, ff_key = jr.split(key, num=3)\n",
        "\n",
        "        self.self_att_block = AttentionBlock(\n",
        "            hidden_size=hidden_size,\n",
        "            num_heads=num_heads,\n",
        "            dropout_rate=dropout_rate,\n",
        "            attention_dropout_rate=attention_dropout_rate,\n",
        "            key=self_att_key,\n",
        "        )\n",
        "\n",
        "        self.cross_att_block = AttentionBlock(\n",
        "            hidden_size=hidden_size,\n",
        "            num_heads=num_heads,\n",
        "            dropout_rate=dropout_rate,\n",
        "            attention_dropout_rate=attention_dropout_rate,\n",
        "            key=cross_att_key,\n",
        "        )\n",
        "\n",
        "        self.ff_block = FeedForwardBlock(\n",
        "            hidden_size=hidden_size,\n",
        "            intermediate_size=intermediate_size,\n",
        "            dropout_rate=dropout_rate,\n",
        "            key=ff_key,\n",
        "        )\n",
        "\n",
        "     def __call__(\n",
        "        self,\n",
        "        inputs: Float[Array, \"seq_len hidden_size\"],\n",
        "        encoder_output,\n",
        "        src_mask,\n",
        "        tgt_mask,\n",
        "        *,\n",
        "        enable_dropout: bool = False,\n",
        "        key,\n",
        "    ):\n",
        "\n",
        "        self_attn_key, cross_attn_key, ff_key = (None, None, None) if key is None else jr.split(key, num=3)\n",
        "\n",
        "        self_attention_output = self.self_att_block(\n",
        "            inputs, inputs, inputs, tgt_mask, enable_dropout=enable_dropout, key=self_attn_key\n",
        "        )\n",
        "\n",
        "        cross_attention_output = self.cross_att_block(\n",
        "            encoder_output, encoder_output, self_attention_output, src_mask, enable_dropout=enable_dropout, key=cross_attn_key\n",
        "        )\n",
        "\n",
        "        mlp_out = self.ff_block(\n",
        "            cross_attention_output, enable_dropout=enable_dropout, ff_key\n",
        "        )\n",
        "\n",
        "        return mlp_out"
      ],
      "metadata": {
        "id": "Uefs4CKaCod6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoder"
      ],
      "metadata": {
        "id": "nJ2NzZHgNxj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(eqx.Module):\n",
        "\n",
        "    embedder_block: InputEmbeddings\n",
        "    pos_embed: SinusoidalPosEmb\n",
        "    layers: List[DecoderLayer]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        hidden_size: int,\n",
        "        seq_len: int,\n",
        "        intermediate_size: int,\n",
        "        num_heads: int,\n",
        "        num_layers: int,\n",
        "        dropout_rate: float,\n",
        "        attention_dropout_rate: float\n",
        "        key: jr.PRNGKey\n",
        "    ):\n",
        "\n",
        "\n",
        "        embedder_key, layer_key = jr.split(key, num=2)\n",
        "\n",
        "        self.embedder_block = InputEmeddings(\n",
        "          hidden_size, vocab_size, embedder_key\n",
        "        )\n",
        "\n",
        "        self.pos_embed = SinusoidalPosEmb(hidden_size, seq_len, dropout_rate)\n",
        "\n",
        "        layer_keys = jr.split(layer_key, num=num_layers)\n",
        "\n",
        "        self.layers = [\n",
        "          DecoderLayer(\n",
        "              hidden_size=hidden_size, intermediate_size=intermediate_size, num_heads=num_heads, dropout_rate=dropout_rate, attention_dropout_rate=attention_dropout_rate, key=layer_key,\n",
        "          )\n",
        "          for layer_key in layer_keys]\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        x,\n",
        "        encoder_output,\n",
        "        src_mask,\n",
        "        tgt_mask,\n",
        "        *,\n",
        "        enable_dropout: bool = False,\n",
        "        key: Optional[jr.PRNGKey] = None,\n",
        "    ):\n",
        "\n",
        "        embed_inputs = self.embedder_block(x)\n",
        "        x = self.pos_embed(embed_inputs, key=key)\n",
        "\n",
        "\n",
        "        for layer, subkey in zip(self.layers, jr.split(key, len(self.layers))):\n",
        "\n",
        "              x = layer(x, encoder_output, src_mask, tgt_mask, subkey)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "JmdNhQa5Nv9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Layer"
      ],
      "metadata": {
        "id": "GEM8TlL_RgQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Out_Projection_Layer(eqx.Module):\n",
        "\n",
        "    proj: eqx.nn.Linear\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model:int,\n",
        "        vocab_size: int,\n",
        "        key: jr.PRNGKey\n",
        "    ):\n",
        "\n",
        "        self.proj = eqx.nn.Linear(in_features=d_model, out_features=vocab_size, key)\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        x\n",
        "    ):\n",
        "        out = jax.vmap(self.proj)(x)\n",
        "        return jax.nn.log_softmax(out, axis=-1)\n"
      ],
      "metadata": {
        "id": "oajMZv-cRPtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer"
      ],
      "metadata": {
        "id": "-j_Sa0VLTDwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(eqx.Module):\n",
        "\n",
        "    encoder: Encoder\n",
        "    decoder: Decoder\n",
        "\n",
        "    out_proj: Out_Projection_Layer\n",
        "\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        config: Mapping,\n",
        "        key: jr.PRNGKey\n",
        "    ):\n",
        "\n",
        "        encoder_key, decoder_key, out_proj_key = jr.split(key, num=3)\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            vocab_size=config[\"src_vocab_size\"],\n",
        "            hidden_size=config[\"hidden_size\"],\n",
        "            seq_len=config[\"src_seq_len\"],\n",
        "            intermediate_size=config[\"intermediate_size\"],\n",
        "            num_layers=config[\"num_hidden_layers\"],\n",
        "            num_heads=config[\"num_attention_heads\"],\n",
        "            dropout_rate=config[\"hidden_dropout_prob\"],\n",
        "            attention_dropout_rate=config[\"attention_dropout_prob\"],\n",
        "            key=encoder_key,\n",
        "        )\n",
        "\n",
        "        self.decoder = Decoder(\n",
        "            vocab_size=config[\"tgt_vocab_size\"],\n",
        "            hidden_size=config[\"hidden_size\"],\n",
        "            seq_len=config[\"tgt_seq_len\"],\n",
        "            intermediate_size=config[\"intermediate_size\"],\n",
        "            num_layers=config[\"num_hidden_layers\"],\n",
        "            num_heads=config[\"num_attention_heads\"],\n",
        "            dropout_rate=config[\"hidden_dropout_prob\"],\n",
        "            attention_dropout_rate=config[\"attention_dropout_prob\"],\n",
        "            key=decoder_key,\n",
        "        )\n",
        "\n",
        "        self.out_proj(\n",
        "            d_model=config[\"hidden_size\"],\n",
        "            vocab_size=config[\"vocab_size\"],\n",
        "            key=out_proj_key)\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        src,\n",
        "        src_mask,\n",
        "        tgt,\n",
        "        tgt_mask,\n",
        "        key\n",
        "    ):\n",
        "        enc_key, dec_key = jr.split(key, num=2)\n",
        "\n",
        "        #encode\n",
        "        enc = self.encoder(src, src_mask, key=enc_key)\n",
        "\n",
        "        #decode\n",
        "        dec = self.decoder(tgt, enc, src_mask, tgt_mask, key=dec_key)\n",
        "\n",
        "        #out projection\n",
        "        out_proj = self.Out_Projection_Layer(dec)\n",
        "\n",
        "        return out_proj"
      ],
      "metadata": {
        "id": "NkkL6sHbTDH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_config = {\n",
        "    \"src_vocab_size\": 11,\n",
        "    \"tgt_vocab_size\": 11,\n",
        "    \"src_seq_len\": 4,\n",
        "    \"tgt_seq_len\": 4,\n",
        "    \"hidden_size\": 128,     #d_model - 128\n",
        "    \"num_hidden_layers\": 2,   #N - 2\n",
        "    \"num_attention_heads\": 2,   #h - 2\n",
        "    \"intermediate_size\": 512,    #d_ff - 512\n",
        "    \"hidden_dropout_prob\": 0.1,\n",
        "    \"attention_dropout_prob\": 0.1,\n",
        "}"
      ],
      "metadata": {
        "id": "dt5dcYSdUdoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference_test():\n",
        "\n",
        "    test_model = Transformer(gtp_config, key)\n",
        ""
      ],
      "metadata": {
        "id": "EJ0_6PSFnb8w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}