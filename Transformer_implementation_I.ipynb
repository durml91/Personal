{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPFCSErlEhs1k6f90ONNgn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/durml91/Personal/blob/Safety/Transformer_implementation_I.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops\n",
        "!pip install equinox"
      ],
      "metadata": {
        "id": "Or-KppZLpwZD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2663cc6-af13-4b59-cc1a-58d8533ac63d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.1\n",
            "Collecting equinox\n",
            "  Downloading equinox-0.10.6-py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.6/125.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jax>=0.4.11 (from equinox)\n",
            "  Downloading jax-0.4.13.tar.gz (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jaxtyping>=0.2.20 (from equinox)\n",
            "  Downloading jaxtyping-0.2.20-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from equinox) (4.6.3)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.11->equinox) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.11->equinox) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.11->equinox) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.11->equinox) (1.10.1)\n",
            "Collecting typeguard>=2.13.3 (from jaxtyping>=0.2.20->equinox)\n",
            "  Downloading typeguard-4.0.0-py3-none-any.whl (33 kB)\n",
            "Building wheels for collected packages: jax\n",
            "  Building wheel for jax (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jax: filename=jax-0.4.13-py3-none-any.whl size=1518707 sha256=783cbce0987d567336350586c82954a49f82cf59434a4818acd282fad66aca99\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/7a/25/f297f69029b5e4064e4736a0c4b3996a44cc27781c120bcb99\n",
            "Successfully built jax\n",
            "Installing collected packages: typeguard, jaxtyping, jax, equinox\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.4.10\n",
            "    Uninstalling jax-0.4.10:\n",
            "      Successfully uninstalled jax-0.4.10\n",
            "Successfully installed equinox-0.10.6 jax-0.4.13 jaxtyping-0.2.20 typeguard-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U jax jaxlib"
      ],
      "metadata": {
        "id": "yIDvm_6XqTu3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfc54257-3210-469e-cb11-3da29f1b246a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (0.4.13)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (0.4.10+cuda11.cudnn86)\n",
            "Collecting jaxlib\n",
            "  Downloading jaxlib-0.4.13-cp310-cp310-manylinux2014_x86_64.whl (71.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from jax) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax) (1.10.1)\n",
            "Installing collected packages: jaxlib\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.4.10+cuda11.cudnn86\n",
            "    Uninstalling jaxlib-0.4.10+cuda11.cudnn86:\n",
            "      Successfully uninstalled jaxlib-0.4.10+cuda11.cudnn86\n",
            "Successfully installed jaxlib-0.4.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IgYFrPvSpbjk"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.random as jr\n",
        "import jax.numpy as jnp\n",
        "import einops\n",
        "import equinox as eqx\n",
        "import optax\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "from typing import Dict, List, Mapping, Optional, Callable\n",
        "\n",
        "#from datasets import load_dataset\n",
        "\n",
        "from jaxtyping import Array, Float, Int\n",
        "\n",
        "from tqdm import notebook as tqdm\n",
        "\n",
        "import math"
      ],
      "metadata": {
        "id": "QEVh-COM17CC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key = jr.PRNGKey(2022)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52URpWXEprjm",
        "outputId": "e94c5185-0727-4733-98b4-7f56ed2bdc28"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer modules"
      ],
      "metadata": {
        "id": "P6oW6sooqpX1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GELU"
      ],
      "metadata": {
        "id": "zokOJ5im3RfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Lambda(eqx.Module):\n",
        "\n",
        "    fn: Callable\n",
        "\n",
        "    def __call__(self, x, *, key=None):\n",
        "        return self.fn(x)"
      ],
      "metadata": {
        "id": "kKw12VA93Q1s"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention"
      ],
      "metadata": {
        "id": "ZX585wtq5F0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionBlock(eqx.Module):\n",
        "\n",
        "    attention: eqx.nn.MultiheadAttention\n",
        "    layernorm: eqx.nn.LayerNorm\n",
        "    dropout: eqx.nn.Dropout\n",
        "    num_heads: int = eqx.field(static=True)\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size: int,\n",
        "        num_heads: int,\n",
        "        dropout_rate: float,\n",
        "        attention_dropout_rate: float,\n",
        "        key: jr.PRNGKey\n",
        "    ):\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = eqx.nn.MultiheadAttention(\n",
        "            num_heads=num_heads,\n",
        "            query_size=hidden_size,\n",
        "            use_query_bias=True,\n",
        "            use_key_bias=True,\n",
        "            use_value_bias=True,\n",
        "            use_output_bias=True,\n",
        "            dropout_p=attention_dropout_rate,\n",
        "            key=key,\n",
        "        )\n",
        "\n",
        "        self.layernorm = eqx.nn.LayerNorm(shape=hidden_size)\n",
        "        self.dropout = eqx.nn.Dropout(dropout_rate)\n",
        "\n",
        "\n",
        "    def make_self_attention_mask(\n",
        "        self, mask: Int[Array, \"seq_len\"]\n",
        "    ) -> Float[Array, \"num_heads seq_len seq_len\"]:\n",
        "\n",
        "        mask = jnp.multiply(\n",
        "            jnp.expand_dims(mask, axis=-1), jnp.expand_dims(mask, axis=-2)\n",
        "        )\n",
        "\n",
        "        #see if you can do this with einops rearrange or repeat (repeat allows you to add any number of dimensions in new axis)\n",
        "\n",
        "        mask = jnp.expand_dims(mask, axis=-3)\n",
        "        mask = jnp.repeat(mask, repeats=self.num_heads, axis=-3)\n",
        "\n",
        "        return mask.astype(jnp.float32)\n",
        "\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        input1: Float[Array, \"seq_len hidden_size\"],\n",
        "        input2: Float[Array, \"seq_len hidden_size\"],\n",
        "        input3: Float[Array, \"seq_len hidden_size\"],\n",
        "        mask: Optional[Int[Array, \"seq_len\"]],\n",
        "        enable_dropout: bool = False,\n",
        "        key: \"jr.PRNGKey\" = None,\n",
        "    ) -> Float[Array, \"seq_len hidden_size\"]:\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = self.make_self_attention_mask(mask)\n",
        "\n",
        "        attention_key, dropout_key = (\n",
        "            (None, None) if key is None else jr.split(key)\n",
        "        )\n",
        "\n",
        "        attention_output = self.attention(\n",
        "              query=input1,\n",
        "              key_=input2,\n",
        "              value=input3,\n",
        "              mask=mask,\n",
        "              inference=not enable_dropout,\n",
        "              key=attention_key\n",
        "        )\n",
        "\n",
        "        att_drop = self.dropout(attention_output, inference=not enable_dropout, key=dropout_key)\n",
        "        unn_out = att_drop + inputs\n",
        "        output = jax.vmap(self.layernorm)(unn_out)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "0PShmTm65HGC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP Block"
      ],
      "metadata": {
        "id": "BsMLjw_gEfsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardBlock(eqx.Module):\n",
        "\n",
        "    mlp: eqx.nn.Sequential    #could also use MLP if this way is more fiddly\n",
        "\n",
        "    layernorm: eqx.nn.LayerNorm\n",
        "    dropout: eqx.nn.Dropout\n",
        "\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size: int,\n",
        "        intermediate_size: int,\n",
        "        dropout_rate: float,\n",
        "        key: jr.PRNGKey,\n",
        "    ):\n",
        "\n",
        "        linear1, linear2 = jr.split(key)\n",
        "\n",
        "        self.mlp = eqx.nn.Sequential([\n",
        "            eqx.nn.Linear(in_features=intermediate_size, out_features=intermediate_size, key=linear1),\n",
        "            Lambda(jax.nn.gelu),\n",
        "            eqx.nn.Linear(in_features=intermediate_size, out_features=hidden_size, key=linear2)\n",
        "        ])\n",
        "\n",
        "        self.layernorm = eqx.nn.LayerNorm(shape=hidden_size)\n",
        "        self.dropout = eqx.nn.Dropout(dropout_rate)\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        inputs: Float[Array, \"seq_len hidden_size\"],\n",
        "        enable_dropout: bool = True,\n",
        "        key: Optional[jr.PRNGKey] = None,\n",
        "    ) -> Float[Array, \"seq_len hidden_size\"]:\n",
        "\n",
        "        feed_out = self.mlp(inputs)\n",
        "\n",
        "        out_d = self.dropout(feed_out, inference=not enable_dropout, key=key)\n",
        "\n",
        "        out_unn = out_d + inputs\n",
        "\n",
        "        output = self.layernorm(out_unn)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "Npn_D1FBEOTi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding table"
      ],
      "metadata": {
        "id": "zvVB7q4OSMvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InputEmbeddings(eqx.Module):\n",
        "\n",
        "  embedding: eqx.nn.Embedding\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      d_model: int,\n",
        "      vocab_size: int,\n",
        "      key: jr.PRNGKey,\n",
        "  ):\n",
        "\n",
        "      self.d_model = d_model\n",
        "      self.vocab_size = vocab_size\n",
        "      self.embedding = eqx.nn.Embedding(vocab_size, d_model, key)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return self.embedding(x) * math.sqrt(self.d_model)"
      ],
      "metadata": {
        "id": "gW98v6F7qjxB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positional encoding"
      ],
      "metadata": {
        "id": "eGzfSNHsSOb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SinusoidalPosEmb(eqx.Module):\n",
        "    pos_emb: jax.Array\n",
        "    dropout: eqx.nn.Dropout\n",
        "\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        seq_len: int,\n",
        "        dropout_rate: float\n",
        "    ):\n",
        "        self.dropout = eqx.nn.Dropout(dropout_rate)\n",
        "\n",
        "        pe = jnp.zeros((seq_len, d_model))\n",
        "        position = einops.repeat(jnp.expand_dims(jnp.arange(0 , seq_len), axis=1), \" s 1 -> s (r 1)\", r=(d_model/2))  #shape [seq_len, d_model/2]\n",
        "        div_term = jnp.exp(jnp.arange(0, d_model, 2) * -(math.log(10_000) / d_model))  #shape [d_model/2]\n",
        "\n",
        "        ins = jax.vmap(jnp.multiply, in_axes=(1, 0), out_axes=1)(position, div_term)  #shape [seq_len, d_model/2]\n",
        "\n",
        "\n",
        "\n",
        "        pe = pe.at[:, 0::2].set(jnp.sin(ins))\n",
        "        pe = pe.at[:, 1::2].set(jnp.cos(ins))\n",
        "\n",
        "\n",
        "        self.pos_emb = jnp.expand_dims(pe, axis=0)\n",
        "\n",
        "        print(self.pos_emb.shape)\n",
        "    def __call__(\n",
        "        self,\n",
        "        x,\n",
        "        enable_dropout: bool = False,\n",
        "        key: \"jr.PRNGKey\" = None,\n",
        "    ) -> Float[Array, \"seq_len d_model\"]:\n",
        "\n",
        "\n",
        "        x = x + self.pos_emb[:, :x.shape[1]]\n",
        "\n",
        "        return self.dropout(x, inference=not enable_dropout, key=key)\n",
        "\n",
        "\n",
        "\n",
        "####### need to freeze parameters use eqx.partition or try filter(static=True)"
      ],
      "metadata": {
        "id": "Ua8tZGvG7pe7"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encod_block = SinusoidalPosEmb(d_model=20, seq_len=5000, dropout_rate=0.0)\n",
        "\n",
        "#pe = encod_block.pos_emb.T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnGMDGQJOrFt",
        "outputId": "5189d22b-9c24-4aff-eef0-0f26ee009a58"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 5000, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"Implement the PE function.\"\n",
        "\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
        "        )\n",
        "\n",
        "        print(div_term.shape)\n",
        "\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        print(pe.shape)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        print(pe.shape)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "OC1-5bJcQJQD"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pe = PositionalEncoding(20, 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uprQO_YgRVQZ",
        "outputId": "337a515d-7fd0-40c6-c01b-a53a85faff6a"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10])\n",
            "torch.Size([5000, 20])\n",
            "torch.Size([1, 5000, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.arange(0, 20, 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkmgeDZhR23a",
        "outputId": "7ee27354-2388-4205-d36a-2bbdf379db42"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import altair as alt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJVlTYOXQcli",
        "outputId": "e92182ae-4e39-494e-be20-f7721b439fc9"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PositionalEncoding(\n",
              "  (dropout): Dropout(p=0, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RUN_EXAMPLES = True"
      ],
      "metadata": {
        "id": "QMCC_QClRLaK"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_example(fn, args=[]):\n",
        "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
        "        return fn(*args)"
      ],
      "metadata": {
        "id": "1xxn51xYRCgn"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def example_positional():\n",
        "    pe = SinusoidalPosEmb(20, 5000, 0)\n",
        "    y = pe(jnp.zeros((1, 100, 20), dtype=jnp.float32))\n",
        "\n",
        "    data = pd.concat(\n",
        "        [\n",
        "            pd.DataFrame(\n",
        "                {\n",
        "                    \"embedding\": y[0, :, dim],\n",
        "                    \"dimension\": dim,\n",
        "                    \"position\": list(range(100)),\n",
        "                }\n",
        "            )\n",
        "            for dim in [4, 5, 6, 7]\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return (\n",
        "        alt.Chart(data)\n",
        "        .mark_line()\n",
        "        .properties(width=800)\n",
        "        .encode(x=\"position\", y=\"embedding\", color=\"dimension:N\")\n",
        "        .interactive()\n",
        "    )\n",
        "\n",
        "show_example(example_positional)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "zGyAZEM2Q11S",
        "outputId": "fbc268e5-c246-4eeb-b4a3-85bb7fdde9ce"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 5000, 20)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<div id=\"altair-viz-dc7a544ef3734c84872bc3092ec5ec33\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-dc7a544ef3734c84872bc3092ec5ec33\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-dc7a544ef3734c84872bc3092ec5ec33\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function maybeLoadScript(lib, version) {\n",
              "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
              "      return (VEGA_DEBUG[key] == version) ?\n",
              "        Promise.resolve(paths[lib]) :\n",
              "        new Promise(function(resolve, reject) {\n",
              "          var s = document.createElement('script');\n",
              "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "          s.async = true;\n",
              "          s.onload = () => {\n",
              "            VEGA_DEBUG[key] = version;\n",
              "            return resolve(paths[lib]);\n",
              "          };\n",
              "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "          s.src = paths[lib];\n",
              "        });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else {\n",
              "      maybeLoadScript(\"vega\", \"5\")\n",
              "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
              "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-a4b8879ab5b4ea982c949659c4cff609\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"field\": \"dimension\", \"type\": \"nominal\"}, \"x\": {\"field\": \"position\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"embedding\", \"type\": \"quantitative\"}}, \"selection\": {\"selector002\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"width\": 800, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-a4b8879ab5b4ea982c949659c4cff609\": [{\"embedding\": 0.0, \"dimension\": 4, \"position\": 0}, {\"embedding\": 0.15782663226127625, \"dimension\": 4, \"position\": 1}, {\"embedding\": 0.3116971552371979, \"dimension\": 4, \"position\": 2}, {\"embedding\": 0.45775455236434937, \"dimension\": 4, \"position\": 3}, {\"embedding\": 0.5923377275466919, \"dimension\": 4, \"position\": 4}, {\"embedding\": 0.7120732069015503, \"dimension\": 4, \"position\": 5}, {\"embedding\": 0.813959538936615, \"dimension\": 4, \"position\": 6}, {\"embedding\": 0.8954429626464844, \"dimension\": 4, \"position\": 7}, {\"embedding\": 0.9544808864593506, \"dimension\": 4, \"position\": 8}, {\"embedding\": 0.989593505859375, \"dimension\": 4, \"position\": 9}, {\"embedding\": 0.9999006390571594, \"dimension\": 4, \"position\": 10}, {\"embedding\": 0.9851439595222473, \"dimension\": 4, \"position\": 11}, {\"embedding\": 0.94569331407547, \"dimension\": 4, \"position\": 12}, {\"embedding\": 0.8825376033782959, \"dimension\": 4, \"position\": 13}, {\"embedding\": 0.7972599267959595, \"dimension\": 4, \"position\": 14}, {\"embedding\": 0.6919978260993958, \"dimension\": 4, \"position\": 15}, {\"embedding\": 0.5693899393081665, \"dimension\": 4, \"position\": 16}, {\"embedding\": 0.4325096309185028, \"dimension\": 4, \"position\": 17}, {\"embedding\": 0.284787654876709, \"dimension\": 4, \"position\": 18}, {\"embedding\": 0.12992730736732483, \"dimension\": 4, \"position\": 19}, {\"embedding\": -0.028190065175294876, \"dimension\": 4, \"position\": 20}, {\"embedding\": -0.18560057878494263, \"dimension\": 4, \"position\": 21}, {\"embedding\": -0.3383587896823883, \"dimension\": 4, \"position\": 22}, {\"embedding\": -0.4826357662677765, \"dimension\": 4, \"position\": 23}, {\"embedding\": -0.6148146390914917, \"dimension\": 4, \"position\": 24}, {\"embedding\": -0.7315824031829834, \"dimension\": 4, \"position\": 25}, {\"embedding\": -0.8300122618675232, \"dimension\": 4, \"position\": 26}, {\"embedding\": -0.9076365828514099, \"dimension\": 4, \"position\": 27}, {\"embedding\": -0.96250981092453, \"dimension\": 4, \"position\": 28}, {\"embedding\": -0.9932565093040466, \"dimension\": 4, \"position\": 29}, {\"embedding\": -0.9991058707237244, \"dimension\": 4, \"position\": 30}, {\"embedding\": -0.9799113273620605, \"dimension\": 4, \"position\": 31}, {\"embedding\": -0.9361540079116821, \"dimension\": 4, \"position\": 32}, {\"embedding\": -0.8689308166503906, \"dimension\": 4, \"position\": 33}, {\"embedding\": -0.7799267172813416, \"dimension\": 4, \"position\": 34}, {\"embedding\": -0.6713724136352539, \"dimension\": 4, \"position\": 35}, {\"embedding\": -0.5459895133972168, \"dimension\": 4, \"position\": 36}, {\"embedding\": -0.40692076086997986, \"dimension\": 4, \"position\": 37}, {\"embedding\": -0.2576519548892975, \"dimension\": 4, \"position\": 38}, {\"embedding\": -0.10192479938268661, \"dimension\": 4, \"position\": 39}, {\"embedding\": 0.056357722729444504, \"dimension\": 4, \"position\": 40}, {\"embedding\": 0.21322709321975708, \"dimension\": 4, \"position\": 41}, {\"embedding\": 0.3647516369819641, \"dimension\": 4, \"position\": 42}, {\"embedding\": 0.5071332454681396, \"dimension\": 4, \"position\": 43}, {\"embedding\": 0.6368028521537781, \"dimension\": 4, \"position\": 44}, {\"embedding\": 0.7505101561546326, \"dimension\": 4, \"position\": 45}, {\"embedding\": 0.8454052805900574, \"dimension\": 4, \"position\": 46}, {\"embedding\": 0.9191088080406189, \"dimension\": 4, \"position\": 47}, {\"embedding\": 0.9697737693786621, \"dimension\": 4, \"position\": 48}, {\"embedding\": 0.9961300492286682, \"dimension\": 4, \"position\": 49}, {\"embedding\": 0.9975170493125916, \"dimension\": 4, \"position\": 50}, {\"embedding\": 0.9738998413085938, \"dimension\": 4, \"position\": 51}, {\"embedding\": 0.9258706569671631, \"dimension\": 4, \"position\": 52}, {\"embedding\": 0.8546332716941833, \"dimension\": 4, \"position\": 53}, {\"embedding\": 0.7619734406471252, \"dimension\": 4, \"position\": 54}, {\"embedding\": 0.6502137184143066, \"dimension\": 4, \"position\": 55}, {\"embedding\": 0.5221555233001709, \"dimension\": 4, \"position\": 56}, {\"embedding\": 0.38100889325141907, \"dimension\": 4, \"position\": 57}, {\"embedding\": 0.23031172156333923, \"dimension\": 4, \"position\": 58}, {\"embedding\": 0.07384055852890015, \"dimension\": 4, \"position\": 59}, {\"embedding\": -0.08448058366775513, \"dimension\": 4, \"position\": 60}, {\"embedding\": -0.2406841218471527, \"dimension\": 4, \"position\": 61}, {\"embedding\": -0.3908545970916748, \"dimension\": 4, \"position\": 62}, {\"embedding\": -0.5312277674674988, \"dimension\": 4, \"position\": 63}, {\"embedding\": -0.6582850813865662, \"dimension\": 4, \"position\": 64}, {\"embedding\": -0.768841564655304, \"dimension\": 4, \"position\": 65}, {\"embedding\": -0.8601260781288147, \"dimension\": 4, \"position\": 66}, {\"embedding\": -0.9298503994941711, \"dimension\": 4, \"position\": 67}, {\"embedding\": -0.9762668013572693, \"dimension\": 4, \"position\": 68}, {\"embedding\": -0.9982118010520935, \"dimension\": 4, \"position\": 69}, {\"embedding\": -0.9951352477073669, \"dimension\": 4, \"position\": 70}, {\"embedding\": -0.967114269733429, \"dimension\": 4, \"position\": 71}, {\"embedding\": -0.9148513078689575, \"dimension\": 4, \"position\": 72}, {\"embedding\": -0.839656412601471, \"dimension\": 4, \"position\": 73}, {\"embedding\": -0.7434144616127014, \"dimension\": 4, \"position\": 74}, {\"embedding\": -0.6285378336906433, \"dimension\": 4, \"position\": 75}, {\"embedding\": -0.4979061186313629, \"dimension\": 4, \"position\": 76}, {\"embedding\": -0.3547937273979187, \"dimension\": 4, \"position\": 77}, {\"embedding\": -0.20278796553611755, \"dimension\": 4, \"position\": 78}, {\"embedding\": -0.04569905996322632, \"dimension\": 4, \"position\": 79}, {\"embedding\": 0.11253630369901657, \"dimension\": 4, \"position\": 80}, {\"embedding\": 0.2679498493671417, \"dimension\": 4, \"position\": 81}, {\"embedding\": 0.4166468679904938, \"dimension\": 4, \"position\": 82}, {\"embedding\": 0.5549001097679138, \"dimension\": 4, \"position\": 83}, {\"embedding\": 0.6792440414428711, \"dimension\": 4, \"position\": 84}, {\"embedding\": 0.7865618467330933, \"dimension\": 4, \"position\": 85}, {\"embedding\": 0.8741634488105774, \"dimension\": 4, \"position\": 86}, {\"embedding\": 0.9398530125617981, \"dimension\": 4, \"position\": 87}, {\"embedding\": 0.9819839596748352, \"dimension\": 4, \"position\": 88}, {\"embedding\": 0.9995002150535583, \"dimension\": 4, \"position\": 89}, {\"embedding\": 0.9919626712799072, \"dimension\": 4, \"position\": 90}, {\"embedding\": 0.959559977054596, \"dimension\": 4, \"position\": 91}, {\"embedding\": 0.903104841709137, \"dimension\": 4, \"position\": 92}, {\"embedding\": 0.8240122199058533, \"dimension\": 4, \"position\": 93}, {\"embedding\": 0.7242646217346191, \"dimension\": 4, \"position\": 94}, {\"embedding\": 0.6063624024391174, \"dimension\": 4, \"position\": 95}, {\"embedding\": 0.4732609689235687, \"dimension\": 4, \"position\": 96}, {\"embedding\": 0.32829657196998596, \"dimension\": 4, \"position\": 97}, {\"embedding\": 0.17510302364826202, \"dimension\": 4, \"position\": 98}, {\"embedding\": 0.01752028614282608, \"dimension\": 4, \"position\": 99}, {\"embedding\": 1.0, \"dimension\": 5, \"position\": 0}, {\"embedding\": 0.9874668121337891, \"dimension\": 5, \"position\": 1}, {\"embedding\": 0.9501814842224121, \"dimension\": 5, \"position\": 2}, {\"embedding\": 0.8890786170959473, \"dimension\": 5, \"position\": 3}, {\"embedding\": 0.805689811706543, \"dimension\": 5, \"position\": 4}, {\"embedding\": 0.7021052241325378, \"dimension\": 5, \"position\": 5}, {\"embedding\": 0.5809215307235718, \"dimension\": 5, \"position\": 6}, {\"embedding\": 0.44517630338668823, \"dimension\": 5, \"position\": 7}, {\"embedding\": 0.2982720732688904, \"dimension\": 5, \"position\": 8}, {\"embedding\": 0.14389123022556305, \"dimension\": 5, \"position\": 9}, {\"embedding\": -0.01409643329679966, \"dimension\": 5, \"position\": 10}, {\"embedding\": -0.1717306226491928, \"dimension\": 5, \"position\": 11}, {\"embedding\": -0.32506027817726135, \"dimension\": 5, \"position\": 12}, {\"embedding\": -0.47024187445640564, \"dimension\": 5, \"position\": 13}, {\"embedding\": -0.6036361455917358, \"dimension\": 5, \"position\": 14}, {\"embedding\": -0.7218996286392212, \"dimension\": 5, \"position\": 15}, {\"embedding\": -0.8220675587654114, \"dimension\": 5, \"position\": 16}, {\"embedding\": -0.9016293287277222, \"dimension\": 5, \"position\": 17}, {\"embedding\": -0.9585906267166138, \"dimension\": 5, \"position\": 18}, {\"embedding\": -0.9915235042572021, \"dimension\": 5, \"position\": 19}, {\"embedding\": -0.9996025562286377, \"dimension\": 5, \"position\": 20}, {\"embedding\": -0.9826252460479736, \"dimension\": 5, \"position\": 21}, {\"embedding\": -0.941017210483551, \"dimension\": 5, \"position\": 22}, {\"embedding\": -0.8758211731910706, \"dimension\": 5, \"position\": 23}, {\"embedding\": -0.788671612739563, \"dimension\": 5, \"position\": 24}, {\"embedding\": -0.6817529797554016, \"dimension\": 5, \"position\": 25}, {\"embedding\": -0.5577451586723328, \"dimension\": 5, \"position\": 26}, {\"embedding\": -0.4197568893432617, \"dimension\": 5, \"position\": 27}, {\"embedding\": -0.27124688029289246, \"dimension\": 5, \"position\": 28}, {\"embedding\": -0.11593768745660782, \"dimension\": 5, \"position\": 29}, {\"embedding\": 0.042278096079826355, \"dimension\": 5, \"position\": 30}, {\"embedding\": 0.19943365454673767, \"dimension\": 5, \"position\": 31}, {\"embedding\": 0.3515901565551758, \"dimension\": 5, \"position\": 32}, {\"embedding\": 0.4949335753917694, \"dimension\": 5, \"position\": 33}, {\"embedding\": 0.6258708238601685, \"dimension\": 5, \"position\": 34}, {\"embedding\": 0.7411201596260071, \"dimension\": 5, \"position\": 35}, {\"embedding\": 0.8377919793128967, \"dimension\": 5, \"position\": 36}, {\"embedding\": 0.9134634733200073, \"dimension\": 5, \"position\": 37}, {\"embedding\": 0.9662377834320068, \"dimension\": 5, \"position\": 38}, {\"embedding\": 0.994792103767395, \"dimension\": 5, \"position\": 39}, {\"embedding\": 0.9984106421470642, \"dimension\": 5, \"position\": 40}, {\"embedding\": 0.9770026803016663, \"dimension\": 5, \"position\": 41}, {\"embedding\": 0.931104838848114, \"dimension\": 5, \"position\": 42}, {\"embedding\": 0.8618676662445068, \"dimension\": 5, \"position\": 43}, {\"embedding\": 0.7710266709327698, \"dimension\": 5, \"position\": 44}, {\"embedding\": 0.6608588695526123, \"dimension\": 5, \"position\": 45}, {\"embedding\": 0.5341253876686096, \"dimension\": 5, \"position\": 46}, {\"embedding\": 0.3940037488937378, \"dimension\": 5, \"position\": 47}, {\"embedding\": 0.24400585889816284, \"dimension\": 5, \"position\": 48}, {\"embedding\": 0.08789165318012238, \"dimension\": 5, \"position\": 49}, {\"embedding\": -0.07042567431926727, \"dimension\": 5, \"position\": 50}, {\"embedding\": -0.2269781529903412, \"dimension\": 5, \"position\": 51}, {\"embedding\": -0.37784066796302795, \"dimension\": 5, \"position\": 52}, {\"embedding\": -0.5192320942878723, \"dimension\": 5, \"position\": 53}, {\"embedding\": -0.6476082801818848, \"dimension\": 5, \"position\": 54}, {\"embedding\": -0.7597513794898987, \"dimension\": 5, \"position\": 55}, {\"embedding\": -0.8528502583503723, \"dimension\": 5, \"position\": 56}, {\"embedding\": -0.9245713949203491, \"dimension\": 5, \"position\": 57}, {\"embedding\": -0.9731168746948242, \"dimension\": 5, \"position\": 58}, {\"embedding\": -0.9972700476646423, \"dimension\": 5, \"position\": 59}, {\"embedding\": -0.9964251518249512, \"dimension\": 5, \"position\": 60}, {\"embedding\": -0.9706035256385803, \"dimension\": 5, \"position\": 61}, {\"embedding\": -0.9204524159431458, \"dimension\": 5, \"position\": 62}, {\"embedding\": -0.8472290635108948, \"dimension\": 5, \"position\": 63}, {\"embedding\": -0.7527687549591064, \"dimension\": 5, \"position\": 64}, {\"embedding\": -0.6394393444061279, \"dimension\": 5, \"position\": 65}, {\"embedding\": -0.5100815296173096, \"dimension\": 5, \"position\": 66}, {\"embedding\": -0.3679378628730774, \"dimension\": 5, \"position\": 67}, {\"embedding\": -0.2165713608264923, \"dimension\": 5, \"position\": 68}, {\"embedding\": -0.05977622792124748, \"dimension\": 5, \"position\": 69}, {\"embedding\": 0.09851823002099991, \"dimension\": 5, \"position\": 70}, {\"embedding\": 0.254342257976532, \"dimension\": 5, \"position\": 71}, {\"embedding\": 0.4037908613681793, \"dimension\": 5, \"position\": 72}, {\"embedding\": 0.543117880821228, \"dimension\": 5, \"position\": 73}, {\"embedding\": 0.6688309907913208, \"dimension\": 5, \"position\": 74}, {\"embedding\": 0.7777789831161499, \"dimension\": 5, \"position\": 75}, {\"embedding\": 0.8672309517860413, \"dimension\": 5, \"position\": 76}, {\"embedding\": 0.9349446296691895, \"dimension\": 5, \"position\": 77}, {\"embedding\": 0.9792226552963257, \"dimension\": 5, \"position\": 78}, {\"embedding\": 0.998955249786377, \"dimension\": 5, \"position\": 79}, {\"embedding\": 0.9936476349830627, \"dimension\": 5, \"position\": 80}, {\"embedding\": 0.9634328484535217, \"dimension\": 5, \"position\": 81}, {\"embedding\": 0.9090684056282043, \"dimension\": 5, \"position\": 82}, {\"embedding\": 0.8319169878959656, \"dimension\": 5, \"position\": 83}, {\"embedding\": 0.733912467956543, \"dimension\": 5, \"position\": 84}, {\"embedding\": 0.617511510848999, \"dimension\": 5, \"position\": 85}, {\"embedding\": 0.4856317937374115, \"dimension\": 5, \"position\": 86}, {\"embedding\": 0.3415791094303131, \"dimension\": 5, \"position\": 87}, {\"embedding\": 0.18896427750587463, \"dimension\": 5, \"position\": 88}, {\"embedding\": 0.0316128134727478, \"dimension\": 5, \"position\": 89}, {\"embedding\": -0.12653106451034546, \"dimension\": 5, \"position\": 90}, {\"embedding\": -0.28150418400764465, \"dimension\": 5, \"position\": 91}, {\"embedding\": -0.4294200837612152, \"dimension\": 5, \"position\": 92}, {\"embedding\": -0.5665720105171204, \"dimension\": 5, \"position\": 93}, {\"embedding\": -0.6895220875740051, \"dimension\": 5, \"position\": 94}, {\"embedding\": -0.7951884269714355, \"dimension\": 5, \"position\": 95}, {\"embedding\": -0.880922257900238, \"dimension\": 5, \"position\": 96}, {\"embedding\": -0.9445747137069702, \"dimension\": 5, \"position\": 97}, {\"embedding\": -0.9845501184463501, \"dimension\": 5, \"position\": 98}, {\"embedding\": -0.9998465180397034, \"dimension\": 5, \"position\": 99}, {\"embedding\": 0.0, \"dimension\": 6, \"position\": 0}, {\"embedding\": 0.06305388361215591, \"dimension\": 6, \"position\": 1}, {\"embedding\": 0.12585683166980743, \"dimension\": 6, \"position\": 2}, {\"embedding\": 0.18815888464450836, \"dimension\": 6, \"position\": 3}, {\"embedding\": 0.24971213936805725, \"dimension\": 6, \"position\": 4}, {\"embedding\": 0.31027159094810486, \"dimension\": 6, \"position\": 5}, {\"embedding\": 0.3695962131023407, \"dimension\": 6, \"position\": 6}, {\"embedding\": 0.4274499714374542, \"dimension\": 6, \"position\": 7}, {\"embedding\": 0.4836025834083557, \"dimension\": 6, \"position\": 8}, {\"embedding\": 0.5378305315971375, \"dimension\": 6, \"position\": 9}, {\"embedding\": 0.5899181365966797, \"dimension\": 6, \"position\": 10}, {\"embedding\": 0.6396579146385193, \"dimension\": 6, \"position\": 11}, {\"embedding\": 0.6868520379066467, \"dimension\": 6, \"position\": 12}, {\"embedding\": 0.7313126921653748, \"dimension\": 6, \"position\": 13}, {\"embedding\": 0.7728629112243652, \"dimension\": 6, \"position\": 14}, {\"embedding\": 0.8113372921943665, \"dimension\": 6, \"position\": 15}, {\"embedding\": 0.8465827703475952, \"dimension\": 6, \"position\": 16}, {\"embedding\": 0.8784590363502502, \"dimension\": 6, \"position\": 17}, {\"embedding\": 0.9068393111228943, \"dimension\": 6, \"position\": 18}, {\"embedding\": 0.9316105246543884, \"dimension\": 6, \"position\": 19}, {\"embedding\": 0.9526742100715637, \"dimension\": 6, \"position\": 20}, {\"embedding\": 0.9699464440345764, \"dimension\": 6, \"position\": 21}, {\"embedding\": 0.9833585619926453, \"dimension\": 6, \"position\": 22}, {\"embedding\": 0.9928570985794067, \"dimension\": 6, \"position\": 23}, {\"embedding\": 0.9984043836593628, \"dimension\": 6, \"position\": 24}, {\"embedding\": 0.999978244304657, \"dimension\": 6, \"position\": 25}, {\"embedding\": 0.9975724220275879, \"dimension\": 6, \"position\": 26}, {\"embedding\": 0.9911965131759644, \"dimension\": 6, \"position\": 27}, {\"embedding\": 0.9808759093284607, \"dimension\": 6, \"position\": 28}, {\"embedding\": 0.9666516184806824, \"dimension\": 6, \"position\": 29}, {\"embedding\": 0.9485803842544556, \"dimension\": 6, \"position\": 30}, {\"embedding\": 0.9267339110374451, \"dimension\": 6, \"position\": 31}, {\"embedding\": 0.9011994004249573, \"dimension\": 6, \"position\": 32}, {\"embedding\": 0.8720782399177551, \"dimension\": 6, \"position\": 33}, {\"embedding\": 0.8394865393638611, \"dimension\": 6, \"position\": 34}, {\"embedding\": 0.8035537600517273, \"dimension\": 6, \"position\": 35}, {\"embedding\": 0.7644230127334595, \"dimension\": 6, \"position\": 36}, {\"embedding\": 0.7222501039505005, \"dimension\": 6, \"position\": 37}, {\"embedding\": 0.6772029399871826, \"dimension\": 6, \"position\": 38}, {\"embedding\": 0.6294605731964111, \"dimension\": 6, \"position\": 39}, {\"embedding\": 0.57921302318573, \"dimension\": 6, \"position\": 40}, {\"embedding\": 0.5266605615615845, \"dimension\": 6, \"position\": 41}, {\"embedding\": 0.4720119535923004, \"dimension\": 6, \"position\": 42}, {\"embedding\": 0.41548484563827515, \"dimension\": 6, \"position\": 43}, {\"embedding\": 0.357304185628891, \"dimension\": 6, \"position\": 44}, {\"embedding\": 0.29770180583000183, \"dimension\": 6, \"position\": 45}, {\"embedding\": 0.23691439628601074, \"dimension\": 6, \"position\": 46}, {\"embedding\": 0.17518411576747894, \"dimension\": 6, \"position\": 47}, {\"embedding\": 0.1127568930387497, \"dimension\": 6, \"position\": 48}, {\"embedding\": 0.04988069087266922, \"dimension\": 6, \"position\": 49}, {\"embedding\": -0.013194027356803417, \"dimension\": 6, \"position\": 50}, {\"embedding\": -0.07621623575687408, \"dimension\": 6, \"position\": 51}, {\"embedding\": -0.1389348804950714, \"dimension\": 6, \"position\": 52}, {\"embedding\": -0.20110084116458893, \"dimension\": 6, \"position\": 53}, {\"embedding\": -0.2624664604663849, \"dimension\": 6, \"position\": 54}, {\"embedding\": -0.3227875530719757, \"dimension\": 6, \"position\": 55}, {\"embedding\": -0.3818237781524658, \"dimension\": 6, \"position\": 56}, {\"embedding\": -0.4393406808376312, \"dimension\": 6, \"position\": 57}, {\"embedding\": -0.49510911107063293, \"dimension\": 6, \"position\": 58}, {\"embedding\": -0.5489069223403931, \"dimension\": 6, \"position\": 59}, {\"embedding\": -0.6005204319953918, \"dimension\": 6, \"position\": 60}, {\"embedding\": -0.6497439742088318, \"dimension\": 6, \"position\": 61}, {\"embedding\": -0.6963817477226257, \"dimension\": 6, \"position\": 62}, {\"embedding\": -0.7402478456497192, \"dimension\": 6, \"position\": 63}, {\"embedding\": -0.7811681628227234, \"dimension\": 6, \"position\": 64}, {\"embedding\": -0.8189795017242432, \"dimension\": 6, \"position\": 65}, {\"embedding\": -0.8535317778587341, \"dimension\": 6, \"position\": 66}, {\"embedding\": -0.8846868872642517, \"dimension\": 6, \"position\": 67}, {\"embedding\": -0.9123212099075317, \"dimension\": 6, \"position\": 68}, {\"embedding\": -0.936324954032898, \"dimension\": 6, \"position\": 69}, {\"embedding\": -0.9566020965576172, \"dimension\": 6, \"position\": 70}, {\"embedding\": -0.9730724096298218, \"dimension\": 6, \"position\": 71}, {\"embedding\": -0.9856699705123901, \"dimension\": 6, \"position\": 72}, {\"embedding\": -0.9943448305130005, \"dimension\": 6, \"position\": 73}, {\"embedding\": -0.9990625381469727, \"dimension\": 6, \"position\": 74}, {\"embedding\": -0.9998041391372681, \"dimension\": 6, \"position\": 75}, {\"embedding\": -0.9965668320655823, \"dimension\": 6, \"position\": 76}, {\"embedding\": -0.9893633723258972, \"dimension\": 6, \"position\": 77}, {\"embedding\": -0.9782225489616394, \"dimension\": 6, \"position\": 78}, {\"embedding\": -0.963188648223877, \"dimension\": 6, \"position\": 79}, {\"embedding\": -0.9443213939666748, \"dimension\": 6, \"position\": 80}, {\"embedding\": -0.9216960668563843, \"dimension\": 6, \"position\": 81}, {\"embedding\": -0.8954026699066162, \"dimension\": 6, \"position\": 82}, {\"embedding\": -0.8655455708503723, \"dimension\": 6, \"position\": 83}, {\"embedding\": -0.8322440981864929, \"dimension\": 6, \"position\": 84}, {\"embedding\": -0.795630156993866, \"dimension\": 6, \"position\": 85}, {\"embedding\": -0.7558501362800598, \"dimension\": 6, \"position\": 86}, {\"embedding\": -0.7130619883537292, \"dimension\": 6, \"position\": 87}, {\"embedding\": -0.6674357056617737, \"dimension\": 6, \"position\": 88}, {\"embedding\": -0.6191535592079163, \"dimension\": 6, \"position\": 89}, {\"embedding\": -0.5684073567390442, \"dimension\": 6, \"position\": 90}, {\"embedding\": -0.5153986215591431, \"dimension\": 6, \"position\": 91}, {\"embedding\": -0.46033912897109985, \"dimension\": 6, \"position\": 92}, {\"embedding\": -0.40344759821891785, \"dimension\": 6, \"position\": 93}, {\"embedding\": -0.34495002031326294, \"dimension\": 6, \"position\": 94}, {\"embedding\": -0.28508007526397705, \"dimension\": 6, \"position\": 95}, {\"embedding\": -0.22407560050487518, \"dimension\": 6, \"position\": 96}, {\"embedding\": -0.1621788740158081, \"dimension\": 6, \"position\": 97}, {\"embedding\": -0.09963719546794891, \"dimension\": 6, \"position\": 98}, {\"embedding\": -0.03669850528240204, \"dimension\": 6, \"position\": 99}, {\"embedding\": 1.0, \"dimension\": 7, \"position\": 0}, {\"embedding\": 0.9980100989341736, \"dimension\": 7, \"position\": 1}, {\"embedding\": 0.992048442363739, \"dimension\": 7, \"position\": 2}, {\"embedding\": 0.9821385741233826, \"dimension\": 7, \"position\": 3}, {\"embedding\": 0.9683201313018799, \"dimension\": 7, \"position\": 4}, {\"embedding\": 0.9506479501724243, \"dimension\": 7, \"position\": 5}, {\"embedding\": 0.9291924834251404, \"dimension\": 7, \"position\": 6}, {\"embedding\": 0.9040390253067017, \"dimension\": 7, \"position\": 7}, {\"embedding\": 0.8752877116203308, \"dimension\": 7, \"position\": 8}, {\"embedding\": 0.8430529832839966, \"dimension\": 7, \"position\": 9}, {\"embedding\": 0.8074630498886108, \"dimension\": 7, \"position\": 10}, {\"embedding\": 0.7686597108840942, \"dimension\": 7, \"position\": 11}, {\"embedding\": 0.7267972826957703, \"dimension\": 7, \"position\": 12}, {\"embedding\": 0.6820423603057861, \"dimension\": 7, \"position\": 13}, {\"embedding\": 0.6345730423927307, \"dimension\": 7, \"position\": 14}, {\"embedding\": 0.5845783352851868, \"dimension\": 7, \"position\": 15}, {\"embedding\": 0.532257080078125, \"dimension\": 7, \"position\": 16}, {\"embedding\": 0.47781768441200256, \"dimension\": 7, \"position\": 17}, {\"embedding\": 0.4214765727519989, \"dimension\": 7, \"position\": 18}, {\"embedding\": 0.36345821619033813, \"dimension\": 7, \"position\": 19}, {\"embedding\": 0.30399325489997864, \"dimension\": 7, \"position\": 20}, {\"embedding\": 0.243318572640419, \"dimension\": 7, \"position\": 21}, {\"embedding\": 0.18167544901371002, \"dimension\": 7, \"position\": 22}, {\"embedding\": 0.11930941045284271, \"dimension\": 7, \"position\": 23}, {\"embedding\": 0.056468550115823746, \"dimension\": 7, \"position\": 24}, {\"embedding\": -0.006597157102078199, \"dimension\": 7, \"position\": 25}, {\"embedding\": -0.06963648647069931, \"dimension\": 7, \"position\": 26}, {\"embedding\": -0.1323987990617752, \"dimension\": 7, \"position\": 27}, {\"embedding\": -0.1946340948343277, \"dimension\": 7, \"position\": 28}, {\"embedding\": -0.25609490275382996, \"dimension\": 7, \"position\": 29}, {\"embedding\": -0.31653639674186707, \"dimension\": 7, \"position\": 30}, {\"embedding\": -0.37571826577186584, \"dimension\": 7, \"position\": 31}, {\"embedding\": -0.43340474367141724, \"dimension\": 7, \"position\": 32}, {\"embedding\": -0.4893665015697479, \"dimension\": 7, \"position\": 33}, {\"embedding\": -0.5433804988861084, \"dimension\": 7, \"position\": 34}, {\"embedding\": -0.5952321887016296, \"dimension\": 7, \"position\": 35}, {\"embedding\": -0.6447150111198425, \"dimension\": 7, \"position\": 36}, {\"embedding\": -0.6916319727897644, \"dimension\": 7, \"position\": 37}, {\"embedding\": -0.7357962727546692, \"dimension\": 7, \"position\": 38}, {\"embedding\": -0.7770324349403381, \"dimension\": 7, \"position\": 39}, {\"embedding\": -0.815176248550415, \"dimension\": 7, \"position\": 40}, {\"embedding\": -0.8500756621360779, \"dimension\": 7, \"position\": 41}, {\"embedding\": -0.8815921545028687, \"dimension\": 7, \"position\": 42}, {\"embedding\": -0.9096000790596008, \"dimension\": 7, \"position\": 43}, {\"embedding\": -0.933988094329834, \"dimension\": 7, \"position\": 44}, {\"embedding\": -0.9546589255332947, \"dimension\": 7, \"position\": 45}, {\"embedding\": -0.971530556678772, \"dimension\": 7, \"position\": 46}, {\"embedding\": -0.9845356941223145, \"dimension\": 7, \"position\": 47}, {\"embedding\": -0.9936226010322571, \"dimension\": 7, \"position\": 48}, {\"embedding\": -0.998755156993866, \"dimension\": 7, \"position\": 49}, {\"embedding\": -0.9999129772186279, \"dimension\": 7, \"position\": 50}, {\"embedding\": -0.9970912933349609, \"dimension\": 7, \"position\": 51}, {\"embedding\": -0.9903014898300171, \"dimension\": 7, \"position\": 52}, {\"embedding\": -0.9795705676078796, \"dimension\": 7, \"position\": 53}, {\"embedding\": -0.9649410843849182, \"dimension\": 7, \"position\": 54}, {\"embedding\": -0.9464714527130127, \"dimension\": 7, \"position\": 55}, {\"embedding\": -0.9242351651191711, \"dimension\": 7, \"position\": 56}, {\"embedding\": -0.8983205556869507, \"dimension\": 7, \"position\": 57}, {\"embedding\": -0.8688308000564575, \"dimension\": 7, \"position\": 58}, {\"embedding\": -0.8358834981918335, \"dimension\": 7, \"position\": 59}, {\"embedding\": -0.7996094226837158, \"dimension\": 7, \"position\": 60}, {\"embedding\": -0.7601531147956848, \"dimension\": 7, \"position\": 61}, {\"embedding\": -0.7176715731620789, \"dimension\": 7, \"position\": 62}, {\"embedding\": -0.6723340749740601, \"dimension\": 7, \"position\": 63}, {\"embedding\": -0.6243206262588501, \"dimension\": 7, \"position\": 64}, {\"embedding\": -0.5738227963447571, \"dimension\": 7, \"position\": 65}, {\"embedding\": -0.5210408568382263, \"dimension\": 7, \"position\": 66}, {\"embedding\": -0.46618568897247314, \"dimension\": 7, \"position\": 67}, {\"embedding\": -0.4094752371311188, \"dimension\": 7, \"position\": 68}, {\"embedding\": -0.3511347472667694, \"dimension\": 7, \"position\": 69}, {\"embedding\": -0.2913972735404968, \"dimension\": 7, \"position\": 70}, {\"embedding\": -0.23049965500831604, \"dimension\": 7, \"position\": 71}, {\"embedding\": -0.1686851680278778, \"dimension\": 7, \"position\": 72}, {\"embedding\": -0.10619935393333435, \"dimension\": 7, \"position\": 73}, {\"embedding\": -0.04329042136669159, \"dimension\": 7, \"position\": 74}, {\"embedding\": 0.019790323451161385, \"dimension\": 7, \"position\": 75}, {\"embedding\": 0.08279230445623398, \"dimension\": 7, \"position\": 76}, {\"embedding\": 0.14546526968479156, \"dimension\": 7, \"position\": 77}, {\"embedding\": 0.20755885541439056, \"dimension\": 7, \"position\": 78}, {\"embedding\": 0.2688263952732086, \"dimension\": 7, \"position\": 79}, {\"embedding\": 0.3290245532989502, \"dimension\": 7, \"position\": 80}, {\"embedding\": 0.3879128098487854, \"dimension\": 7, \"position\": 81}, {\"embedding\": 0.4452572762966156, \"dimension\": 7, \"position\": 82}, {\"embedding\": 0.5008301138877869, \"dimension\": 7, \"position\": 83}, {\"embedding\": 0.5544094443321228, \"dimension\": 7, \"position\": 84}, {\"embedding\": 0.605782687664032, \"dimension\": 7, \"position\": 85}, {\"embedding\": 0.6547446846961975, \"dimension\": 7, \"position\": 86}, {\"embedding\": 0.7011010050773621, \"dimension\": 7, \"position\": 87}, {\"embedding\": 0.7446674108505249, \"dimension\": 7, \"position\": 88}, {\"embedding\": 0.7852699160575867, \"dimension\": 7, \"position\": 89}, {\"embedding\": 0.8227472901344299, \"dimension\": 7, \"position\": 90}, {\"embedding\": 0.856950581073761, \"dimension\": 7, \"position\": 91}, {\"embedding\": 0.8877431154251099, \"dimension\": 7, \"position\": 92}, {\"embedding\": 0.9150027632713318, \"dimension\": 7, \"position\": 93}, {\"embedding\": 0.9386210441589355, \"dimension\": 7, \"position\": 94}, {\"embedding\": 0.9585037231445312, \"dimension\": 7, \"position\": 95}, {\"embedding\": 0.9745717644691467, \"dimension\": 7, \"position\": 96}, {\"embedding\": 0.9867613911628723, \"dimension\": 7, \"position\": 97}, {\"embedding\": 0.9950238466262817, \"dimension\": 7, \"position\": 98}, {\"embedding\": 0.9993264079093933, \"dimension\": 7, \"position\": 99}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ],
            "text/plain": [
              "alt.Chart(...)"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "plt.set_cmap('cividis')\n",
        "%matplotlib inline\n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf')\n",
        "from matplotlib.colors import to_rgb\n",
        "matplotlib.rcParams['lines.linewidth']=2.0\n",
        "import seaborn as sns\n",
        "sns.reset_orig()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "qWri-TnwN3io",
        "outputId": "b2450bec-201e-414b-a9f9-f941e38b21ed"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-e9e448fab539>:6: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
            "  set_matplotlib_formats('svg', 'pdf')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer Block"
      ],
      "metadata": {
        "id": "kPL-N55ntQmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerLayer(eqx.Module):\n",
        "\n",
        "    attention_block: AttentionBlock\n",
        "    ff_block: FeedForwardBlock\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size: int,\n",
        "        intermediate_size: int,\n",
        "        num_heads: int,\n",
        "        dropout_rate: float,\n",
        "        attention_dropout_rate: float,\n",
        "        key: jr.PRNGKey,\n",
        "    ):\n",
        "\n",
        "        attention_key, ff_key = jr.split(key)\n",
        "\n",
        "        self.attention_block = AttentionBlock(\n",
        "            hidden_size=hidden_size,\n",
        "            num_heads=num_heads,\n",
        "            dropout_rate=dropout_rate,\n",
        "            attention_dropout_rate=attention_dropout_rate,\n",
        "            key=attention_key,\n",
        "        )\n",
        "\n",
        "        self.ff_block = FeedForwardBlock(\n",
        "            hidden_size=hidden_size,\n",
        "            intermediate_size=intermediate_size,\n",
        "            dropout_rate=dropout_rate,\n",
        "            key=ff_key,\n",
        "        )\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        inputs: Float[Array, \"seq_len hidden_size\"],\n",
        "        mask: Optional[Int[Array, \"seq_len\"]] = None,\n",
        "        *,\n",
        "        enable_dropout: bool = False,\n",
        "        key: Optional[jr.PRNGKey] = None,\n",
        "    ) -> Float[Array, \"seq_len hidden_size\"]:\n",
        "\n",
        "        attn_key, ff_key = (None, None) if key is None else jr.split(key)\n",
        "\n",
        "        attention_output = self.attention_block(\n",
        "            inputs, inputs, inputs, mask, enable_dropout=enable_dropout, key=attn_key\n",
        "        )\n",
        "\n",
        "        mlp_out = self.ff_block(\n",
        "            inputs, enable_dropout=enable_dropout, key=ff_key\n",
        "        )\n",
        "\n",
        "        return mlp_out"
      ],
      "metadata": {
        "id": "s_zpRdXMtP9_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder"
      ],
      "metadata": {
        "id": "7ZDEK-86ym-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(eqx.Module):\n",
        "\n",
        "    embedder_block: InputEmbeddings\n",
        "    pos_embed: SinusoidalPosEmb\n",
        "    layers: List[TransformerLayer]\n",
        "\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        hidden_size: int,\n",
        "        seq_len: int,\n",
        "        intermediate_size: int,\n",
        "        num_layers: int,\n",
        "        num_heads: int,\n",
        "        dropout_rate: float,\n",
        "        attention_dropout_rate: float,\n",
        "        key: jr.PRNGKey,\n",
        "    ):\n",
        "\n",
        "      embedder_key, layer_key = jr.split(key, num=2)\n",
        "\n",
        "      self.embedder_block = InputEmbeddings(\n",
        "          hidden_size, vocab_size, embedder_key\n",
        "      )\n",
        "\n",
        "      self.pos_embed = SinusoidalPosEmb(hidden_size, seq_len, dropout_rate)\n",
        "\n",
        "      layer_keys = jr.split(layer_key, num=num_layers)\n",
        "\n",
        "      self.layers = [\n",
        "          TransformerLayer(\n",
        "              hidden_size=hidden_size, intermediate_size=intermediate_size, num_heads=num_heads, dropout_rate=dropout_rate, attention_dropout_rate=attention_dropout_rate, key=layer_key,\n",
        "          )\n",
        "          for layer_key in layer_keys]\n",
        "\n",
        "\n",
        "\n",
        "      def __call__(\n",
        "          self,\n",
        "          tokens: Int[Array, \" seq_len\"],\n",
        "          mask,\n",
        "          *,\n",
        "          enable_dropout: bool = False,\n",
        "          key: Optional[jr.PRNGKey] = None,\n",
        "      ):\n",
        "\n",
        "          embed_inputs = self.embedder_block(tokens)\n",
        "          x = self.pos_embed(embed_inputs, key=key)\n",
        "\n",
        "          for layer in self.layers:\n",
        "\n",
        "              x = layer(x, mask)\n",
        "\n",
        "          return x"
      ],
      "metadata": {
        "id": "3Szh7x1sylML"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoder Block"
      ],
      "metadata": {
        "id": "7Ayup6V6FxC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(eqx.Module):\n",
        "\n",
        "    self_att_block: AttentionBlock\n",
        "    cross_att_block: AttentionBlock\n",
        "    ff_block: FeedForwardBlock\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size: int,\n",
        "        intermediate_size: int,\n",
        "        num_heads: int,\n",
        "        dropout_rate: float,\n",
        "        attention_dropout_rate: float,\n",
        "        key: jr.PRNGKey,\n",
        "    ):\n",
        "\n",
        "        self_att_key, cross_att_key, ff_key = jr.split(key, num=3)\n",
        "\n",
        "        self.self_att_block = AttentionBlock(\n",
        "            hidden_size=hidden_size,\n",
        "            num_heads=num_heads,\n",
        "            dropout_rate=dropout_rate,\n",
        "            attention_dropout_rate=attention_dropout_rate,\n",
        "            key=self_att_key,\n",
        "        )\n",
        "\n",
        "        self.cross_att_block = AttentionBlock(\n",
        "            hidden_size=hidden_size,\n",
        "            num_heads=num_heads,\n",
        "            dropout_rate=dropout_rate,\n",
        "            attention_dropout_rate=attention_dropout_rate,\n",
        "            key=cross_att_key,\n",
        "        )\n",
        "\n",
        "        self.ff_block = FeedForwardBlock(\n",
        "            hidden_size=hidden_size,\n",
        "            intermediate_size=intermediate_size,\n",
        "            dropout_rate=dropout_rate,\n",
        "            key=ff_key,\n",
        "        )\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        inputs: Float[Array, \"seq_len hidden_size\"],\n",
        "        encoder_output,\n",
        "        src_mask,\n",
        "        tgt_mask,\n",
        "        *,\n",
        "        enable_dropout: bool = False,\n",
        "        key,\n",
        "    ):\n",
        "\n",
        "        self_attn_key, cross_attn_key, ff_key = (None, None, None) if key is None else jr.split(key, num=3)\n",
        "\n",
        "        self_attention_output = self.self_att_block(\n",
        "            inputs, inputs, inputs, tgt_mask, enable_dropout=enable_dropout, key=self_attn_key\n",
        "        )\n",
        "\n",
        "        cross_attention_output = self.cross_att_block(\n",
        "            encoder_output, encoder_output, self_attention_output, src_mask, enable_dropout=enable_dropout, key=cross_attn_key\n",
        "        )\n",
        "\n",
        "        mlp_out = self.ff_block(\n",
        "            cross_attention_output, enable_dropout=enable_dropout, key=ff_key\n",
        "        )\n",
        "\n",
        "        return mlp_out"
      ],
      "metadata": {
        "id": "Uefs4CKaCod6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoder"
      ],
      "metadata": {
        "id": "nJ2NzZHgNxj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(eqx.Module):\n",
        "\n",
        "    embedder_block: InputEmbeddings\n",
        "    pos_embed: SinusoidalPosEmb\n",
        "    layers: List[DecoderLayer]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        hidden_size: int,\n",
        "        seq_len: int,\n",
        "        intermediate_size: int,\n",
        "        num_heads: int,\n",
        "        num_layers: int,\n",
        "        dropout_rate: float,\n",
        "        attention_dropout_rate: float,\n",
        "        key: jr.PRNGKey\n",
        "    ):\n",
        "\n",
        "\n",
        "        embedder_key, layer_key = jr.split(key, num=2)\n",
        "\n",
        "        self.embedder_block = InputEmbeddings(\n",
        "          hidden_size, vocab_size, embedder_key\n",
        "        )\n",
        "\n",
        "        self.pos_embed = SinusoidalPosEmb(hidden_size, seq_len, dropout_rate)\n",
        "\n",
        "        layer_keys = jr.split(layer_key, num=num_layers)\n",
        "\n",
        "        self.layers = [\n",
        "          DecoderLayer(\n",
        "              hidden_size=hidden_size, intermediate_size=intermediate_size, num_heads=num_heads, dropout_rate=dropout_rate, attention_dropout_rate=attention_dropout_rate, key=layer_key,\n",
        "          )\n",
        "          for layer_key in layer_keys]\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        x,\n",
        "        encoder_output,\n",
        "        src_mask,\n",
        "        tgt_mask,\n",
        "        *,\n",
        "        enable_dropout: bool = False,\n",
        "        key: Optional[jr.PRNGKey] = None,\n",
        "    ):\n",
        "\n",
        "        embed_inputs = self.embedder_block(x)\n",
        "        x = self.pos_embed(embed_inputs, key=key)\n",
        "\n",
        "\n",
        "        for layer, subkey in zip(self.layers, jr.split(key, len(self.layers))):\n",
        "\n",
        "              x = layer(x, encoder_output, src_mask, tgt_mask, subkey)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "JmdNhQa5Nv9V"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Layer"
      ],
      "metadata": {
        "id": "GEM8TlL_RgQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Out_Projection_Layer(eqx.Module):\n",
        "\n",
        "    proj: eqx.nn.Linear\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model:int,\n",
        "        vocab_size: int,\n",
        "        key: jr.PRNGKey\n",
        "    ):\n",
        "\n",
        "        self.proj = eqx.nn.Linear(in_features=d_model, out_features=vocab_size, key=key)\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        x\n",
        "    ):\n",
        "        out = jax.vmap(self.proj)(x)\n",
        "        return jax.nn.log_softmax(out, axis=-1)\n"
      ],
      "metadata": {
        "id": "oajMZv-cRPtN"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer"
      ],
      "metadata": {
        "id": "-j_Sa0VLTDwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(eqx.Module):\n",
        "\n",
        "    encoder: Encoder\n",
        "    decoder: Decoder\n",
        "\n",
        "    out_proj: Out_Projection_Layer\n",
        "\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        config: Mapping,\n",
        "        key: jr.PRNGKey\n",
        "    ):\n",
        "\n",
        "        encoder_key, decoder_key, out_proj_key = jr.split(key, num=3)\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            vocab_size=config[\"src_vocab_size\"],\n",
        "            hidden_size=config[\"hidden_size\"],\n",
        "            seq_len=config[\"src_seq_len\"],\n",
        "            intermediate_size=config[\"intermediate_size\"],\n",
        "            num_layers=config[\"num_hidden_layers\"],\n",
        "            num_heads=config[\"num_attention_heads\"],\n",
        "            dropout_rate=config[\"hidden_dropout_prob\"],\n",
        "            attention_dropout_rate=config[\"attention_dropout_prob\"],\n",
        "            key=encoder_key,\n",
        "        )\n",
        "\n",
        "        self.decoder = Decoder(\n",
        "            vocab_size=config[\"tgt_vocab_size\"],\n",
        "            hidden_size=config[\"hidden_size\"],\n",
        "            seq_len=config[\"tgt_seq_len\"],\n",
        "            intermediate_size=config[\"intermediate_size\"],\n",
        "            num_layers=config[\"num_hidden_layers\"],\n",
        "            num_heads=config[\"num_attention_heads\"],\n",
        "            dropout_rate=config[\"hidden_dropout_prob\"],\n",
        "            attention_dropout_rate=config[\"attention_dropout_prob\"],\n",
        "            key=decoder_key,\n",
        "        )\n",
        "\n",
        "        self.out_proj(\n",
        "            d_model=config[\"hidden_size\"],\n",
        "            vocab_size=config[\"vocab_size\"],\n",
        "            key=out_proj_key)\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        src,\n",
        "        src_mask,\n",
        "        tgt,\n",
        "        tgt_mask,\n",
        "        key\n",
        "    ):\n",
        "        enc_key, dec_key = jr.split(key, num=2)\n",
        "\n",
        "        #encode\n",
        "        enc = self.encoder(src, src_mask, key=enc_key)\n",
        "\n",
        "        #decode\n",
        "        dec = self.decoder(tgt, enc, src_mask, tgt_mask, key=dec_key)\n",
        "\n",
        "        #out projection\n",
        "        out_proj = self.Out_Projection_Layer(dec)\n",
        "\n",
        "        return out_proj"
      ],
      "metadata": {
        "id": "NkkL6sHbTDH2"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configuration"
      ],
      "metadata": {
        "id": "r5v38Z0BMQby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_config = {\n",
        "    \"src_vocab_size\": 11,\n",
        "    \"tgt_vocab_size\": 11,\n",
        "    \"src_seq_len\": 4,\n",
        "    \"tgt_seq_len\": 4,\n",
        "    \"hidden_size\": 128,     #d_model - 128\n",
        "    \"num_hidden_layers\": 2,   #N - 2\n",
        "    \"num_attention_heads\": 2,   #h - 2\n",
        "    \"intermediate_size\": 512,    #d_ff - 512\n",
        "    \"hidden_dropout_prob\": 0.1,\n",
        "    \"attention_dropout_prob\": 0.1,\n",
        "}"
      ],
      "metadata": {
        "id": "dt5dcYSdUdoe"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test"
      ],
      "metadata": {
        "id": "yvkhSKr-MPEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_model = Transformer(gpt_config, key)"
      ],
      "metadata": {
        "id": "EJ0_6PSFnb8w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}