{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNBxKa3hXmhKXNLOpVhu1x6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/durml91/Personal/blob/main/Transformer_implementation_I.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops\n",
        "!pip install equinox"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Or-KppZLpwZD",
        "outputId": "144f25e9-b00b-476b-ebb3-9722232a886c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.1\n",
            "Collecting equinox\n",
            "  Downloading equinox-0.10.8-py3-none-any.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jax>=0.4.13 (from equinox)\n",
            "  Downloading jax-0.4.13.tar.gz (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jaxtyping>=0.2.20 (from equinox)\n",
            "  Downloading jaxtyping-0.2.20-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from equinox) (4.6.3)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->equinox) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->equinox) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->equinox) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->equinox) (1.10.1)\n",
            "Collecting typeguard>=2.13.3 (from jaxtyping>=0.2.20->equinox)\n",
            "  Downloading typeguard-4.0.0-py3-none-any.whl (33 kB)\n",
            "Building wheels for collected packages: jax\n",
            "  Building wheel for jax (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jax: filename=jax-0.4.13-py3-none-any.whl size=1518707 sha256=336f3933b2a65be24a9abf6e8ec3a1f8a59a977563c41aca8456b968ac24186d\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/7a/25/f297f69029b5e4064e4736a0c4b3996a44cc27781c120bcb99\n",
            "Successfully built jax\n",
            "Installing collected packages: typeguard, jaxtyping, jax, equinox\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.4.10\n",
            "    Uninstalling jax-0.4.10:\n",
            "      Successfully uninstalled jax-0.4.10\n",
            "Successfully installed equinox-0.10.8 jax-0.4.13 jaxtyping-0.2.20 typeguard-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U jax jaxlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIDvm_6XqTu3",
        "outputId": "dd2ffd95-3124-4621-dcf5-ba9814695511"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (0.4.13)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (0.4.10+cuda11.cudnn86)\n",
            "Collecting jaxlib\n",
            "  Downloading jaxlib-0.4.13-cp310-cp310-manylinux2014_x86_64.whl (71.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from jax) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax) (1.10.1)\n",
            "Installing collected packages: jaxlib\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.4.10+cuda11.cudnn86\n",
            "    Uninstalling jaxlib-0.4.10+cuda11.cudnn86:\n",
            "      Successfully uninstalled jaxlib-0.4.10+cuda11.cudnn86\n",
            "Successfully installed jaxlib-0.4.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IgYFrPvSpbjk"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.random as jr\n",
        "import jax.numpy as jnp\n",
        "import einops\n",
        "import equinox as eqx\n",
        "import optax\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "from typing import Dict, List, Mapping, Optional, Callable\n",
        "\n",
        "#from datasets import load_dataset\n",
        "\n",
        "from jaxtyping import Array, Float, Int\n",
        "\n",
        "from tqdm import notebook as tqdm\n"
      ],
      "metadata": {
        "id": "QEVh-COM17CC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key = jr.PRNGKey(2022)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52URpWXEprjm",
        "outputId": "989a0a0c-bbe1-497c-e0be-f36650aee5af"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer modules"
      ],
      "metadata": {
        "id": "P6oW6sooqpX1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GELU"
      ],
      "metadata": {
        "id": "zokOJ5im3RfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Lambda(eqx.Module):\n",
        "\n",
        "    fn: Callable\n",
        "\n",
        "    def __call__(self, x, *, key=None):\n",
        "        return self.fn(x)"
      ],
      "metadata": {
        "id": "kKw12VA93Q1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# eqx.nn.Sequential([\n",
        "#     eqx.nn.Linear(...),\n",
        "#     Lambda(jax.nn.gelu),\n",
        "#     eqx.nn.Linear(...)\n",
        "# ])"
      ],
      "metadata": {
        "id": "-8XoGxZE4HnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention"
      ],
      "metadata": {
        "id": "ZX585wtq5F0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionBlock(eqx.Module):\n",
        "\n",
        "    attention: eqx.nn.MultiheadAttention\n",
        "    layernorm: eqx.nn.LayerNorm\n",
        "    dropout: eqx.nn.Dropout\n",
        "    num_heads: int = eqx.field(static=True)\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size: int,\n",
        "        num_heads: int,\n",
        "        dropout_rate: float,\n",
        "        attention_dropout_rate: float,\n",
        "        key: jr.PRNGKey\n",
        "    ):\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = eqx.nn.MultiheadAttention(\n",
        "            num_heads=num_heads,\n",
        "            query_size=hidden_size,\n",
        "            use_query_bias=True,\n",
        "            use_key_bias=True,\n",
        "            use_value_bias=True,\n",
        "            use_output_bias=True,\n",
        "            dropout_p=attention_dropout_rate,\n",
        "            key=key,\n",
        "        )\n",
        "\n",
        "        self.layernorm = eqx.nn.LayerNorm(shape=hidden_size)\n",
        "        self.dropout = eqx.nn.Dropout(dropout_rate)\n",
        "\n",
        "\n",
        "    def make_self_attention_mask(\n",
        "        self, mask: Int[Array, \"seq_len\"]\n",
        "    ) -> Float[Array, \"num_heads seq_len seq_len\"]:\n",
        "\n",
        "        mask = jnp.multiply(\n",
        "            jnp.expand_dims(mask, axis=-1), jnp.expand_dims(mask, axis=-2)\n",
        "        )\n",
        "\n",
        "        #see if you can do this with einops rearrange or repeat (repeat allows you to add any number of dimensions in new axis)\n",
        "\n",
        "        mask = jnp.expand_dims(mask, axis=-3)\n",
        "        mask = jnp.repeat(mask, repeats=self.num_heads, axis=-3)\n",
        "\n",
        "        return mask.astype(jnp.float32)\n",
        "\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        inputs: Float[Array, \"seq_len hidden_size\"],\n",
        "        mask: Optional[Int[Array, \"seq_len\"]],\n",
        "        enable_dropout: bool = False,\n",
        "        key: \"jr.PRNGKey\" = None,\n",
        "    ) -> Float[Array, \"seq_len hidden_size\"]:\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = self.make_self_attention_mask(mask)\n",
        "\n",
        "        attention_key, dropout_key = (\n",
        "            (None, None) if key is None else jr.split(key)\n",
        "        )\n",
        "\n",
        "        attention_output = self.attention(\n",
        "              query=inputs,\n",
        "              key_=inputs,\n",
        "              value=inputs,\n",
        "              mask=mask,\n",
        "              inference=not enable_dropout,\n",
        "              key=attention_key\n",
        "        )\n",
        "\n",
        "        att_drop = self.dropout(attention_output, inference=not enable_dropout, key=dropout_key)\n",
        "        unn_out = att_drop + inputs\n",
        "        output = jax.vmap(self.layernorm)(unn_out)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "0PShmTm65HGC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding table"
      ],
      "metadata": {
        "id": "zvVB7q4OSMvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InputEmbeddings(eqx.Module):\n",
        "\n",
        "  embedding: eqx.nn.Embedding\n",
        "\n",
        "  def __init__(self, d_model: int, vocab_size: int, key):\n",
        "\n",
        "      self.d_model = d_model\n",
        "      self.vocab_size = vocab_size\n",
        "      self.embedding = eqx.nn.Embedding(vocab_size, d_model, key)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return self.embedding(x)* jnp.sqrt(self.d_model)"
      ],
      "metadata": {
        "id": "gW98v6F7qjxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positional encoding"
      ],
      "metadata": {
        "id": "eGzfSNHsSOb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(eqx.Module):\n",
        "\n",
        "    dropout: eqx.nn.Dropout\n",
        "\n",
        "    def __init__(self, d_model: int, seq_len: int, dropout_rate:float, key) -> None:\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.seq_len = seq_len\n",
        "        self.dropout = eqx.nn.Dropout(dropout_rate)\n",
        "\n",
        "        pe = jnp.zeros((seq_len, d_model), dtype = float)\n",
        "\n",
        "        position = jnp.arange(0, seq_len, dtype=float)\n",
        "        position = jnp.expand_dims(position, 1)\n",
        "\n",
        "        div_term = jnp.exp(jnp.arange(0, d_model, 2) * (-jnp.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = jnp.sin(position * div_term)\n",
        "        pe[:, 1::2] = jnp.cos(position * div_term)\n",
        "\n",
        "        pe = jnp.expand_dims(pe, 0)\n",
        "\n",
        "    def __call__(self, x):\n",
        "\n",
        "        x = x + (self.pe[:, :x.shape[1], :])\n",
        "\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "####### need to freeze parameters"
      ],
      "metadata": {
        "id": "hAbfEY_4rIkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalisation(eqx.Module):\n",
        "\n",
        "    def __init__(self, eps: float)"
      ],
      "metadata": {
        "id": "DEDKZYRkX-kC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}